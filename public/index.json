[
 {
   "uri": "/posts/sem-diagrams-for-lavaan-models/",
   "title": "SEM Plots for Lavaan Models",
   "tags": ["SEM", "R", "ggplot2", "lavaan"],
   "description": "Drawing path diagrams from lavaan objects in ggplot2",
   "content": "Introduciton In this post I show how to make a nice looking SEM diagram from a model object fitted with lavaan.\nlibrary(tidyverse) library(lavaan) library(ggnetwork)  Lavaan Model Below is the SEM model we are going to fit (from the lavaan website).\n# Lavaan Model model \u0026lt;- \u0026#39; # measurement model ind60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8 # regressions dem60 ~ ind60 dem65 ~ ind60 + dem60 # residual correlations y1 ~~ y5 y2 ~~ y4 + y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 \u0026#39; fit \u0026lt;- sem(model, data=PoliticalDemocracy) Here is the output:\nsummary(fit, standardized=TRUE) ## lavaan 0.6-3 ended normally after 68 iterations ## ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 75 ## ## Estimator ML ## Model Fit Test Statistic 38.125 ## Degrees of freedom 35 ## P-value (Chi-square) 0.329 ## ## Parameter Estimates: ## ## Information Expected ## Information saturated (h1) model Structured ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## ind60 =~ ## x1 1.000 0.670 0.920 ## x2 2.180 0.139 15.742 0.000 1.460 0.973 ## x3 1.819 0.152 11.967 0.000 1.218 0.872 ## dem60 =~ ## y1 1.000 2.223 0.850 ## y2 1.257 0.182 6.889 0.000 2.794 0.717 ## y3 1.058 0.151 6.987 0.000 2.351 0.722 ## y4 1.265 0.145 8.722 0.000 2.812 0.846 ## dem65 =~ ## y5 1.000 2.103 0.808 ## y6 1.186 0.169 7.024 0.000 2.493 0.746 ## y7 1.280 0.160 8.002 0.000 2.691 0.824 ## y8 1.266 0.158 8.007 0.000 2.662 0.828 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## dem60 ~ ## ind60 1.483 0.399 3.715 0.000 0.447 0.447 ## dem65 ~ ## ind60 0.572 0.221 2.586 0.010 0.182 0.182 ## dem60 0.837 0.098 8.514 0.000 0.885 0.885 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .y1 ~~ ## .y5 0.624 0.358 1.741 0.082 0.624 0.296 ## .y2 ~~ ## .y4 1.313 0.702 1.871 0.061 1.313 0.273 ## .y6 2.153 0.734 2.934 0.003 2.153 0.356 ## .y3 ~~ ## .y7 0.795 0.608 1.308 0.191 0.795 0.191 ## .y4 ~~ ## .y8 0.348 0.442 0.787 0.431 0.348 0.109 ## .y6 ~~ ## .y8 1.356 0.568 2.386 0.017 1.356 0.338 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) Std.lv Std.all ## .x1 0.082 0.019 4.184 0.000 0.082 0.154 ## .x2 0.120 0.070 1.718 0.086 0.120 0.053 ## .x3 0.467 0.090 5.177 0.000 0.467 0.239 ## .y1 1.891 0.444 4.256 0.000 1.891 0.277 ## .y2 7.373 1.374 5.366 0.000 7.373 0.486 ## .y3 5.067 0.952 5.324 0.000 5.067 0.478 ## .y4 3.148 0.739 4.261 0.000 3.148 0.285 ## .y5 2.351 0.480 4.895 0.000 2.351 0.347 ## .y6 4.954 0.914 5.419 0.000 4.954 0.443 ## .y7 3.431 0.713 4.814 0.000 3.431 0.322 ## .y8 3.254 0.695 4.685 0.000 3.254 0.315 ## ind60 0.448 0.087 5.173 0.000 1.000 1.000 ## .dem60 3.956 0.921 4.295 0.000 0.800 0.800 ## .dem65 0.172 0.215 0.803 0.422 0.039 0.039  Create Nodes Now we are going to create a nice data.frame to specify the locations of nodes (variables in the SEM model) and edges (paths connecting nodes). First, define where the nodes should be positioned spatially and create a data.frame to hold these data:\nlavaan_parameters \u0026lt;- parameterestimates(fit) nodes \u0026lt;- lavaan_parameters %\u0026gt;% select(lhs) %\u0026gt;% rename(name = lhs) %\u0026gt;% distinct(name) %\u0026gt;% mutate( x = case_when(str_detect(name,(\u0026quot;^y\u0026quot;)) ~ 0, name %in% c(\u0026quot;dem60\u0026quot;,\u0026quot;dem65\u0026quot;) ~ .33, name == \u0026quot;ind60\u0026quot; ~ .66, name == \u0026quot;x1\u0026quot; ~ .6, name == \u0026quot;x2\u0026quot; ~ .66, name == \u0026quot;x3\u0026quot; ~ .72), y = case_when(name %in% c(\u0026quot;x1\u0026quot;,\u0026quot;x2\u0026quot;,\u0026quot;x3\u0026quot;) ~ 1.05, name == \u0026quot;y1\u0026quot; ~ 1.05, name == \u0026quot;y2\u0026quot; ~ .9, name %in% c(\u0026quot;y3\u0026quot;,\u0026quot;dem60\u0026quot;) ~ .75, name == \u0026quot;ind60\u0026quot; ~ .525, name == \u0026quot;y4\u0026quot; ~ .6, name == \u0026quot;y5\u0026quot; ~ .45, name %in% c(\u0026quot;y6\u0026quot;,\u0026quot;dem65\u0026quot;) ~ .3, name == \u0026quot;y7\u0026quot; ~ .15, name == \u0026quot;y8\u0026quot; ~ 0), xend = x, yend = y )  Create Edges Now the same for edges:\nedges \u0026lt;- lavaan_parameters %\u0026gt;% filter(op %in% c(\u0026quot;~\u0026quot;,\u0026quot;=~\u0026quot;)) Next we need to combine our nodes and edges into a single table so we can plot it with ggplot2. To do this, we will merge the nodes and edges in a specific way to get all information represented in a single data.frame:\ncombined \u0026lt;- nodes %\u0026gt;% bind_rows( left_join(edges,nodes %\u0026gt;% select(name,x,y),by=c(\u0026quot;lhs\u0026quot;=\u0026quot;name\u0026quot;)) %\u0026gt;% left_join(nodes %\u0026gt;% select(name,xend,yend),by = c(\u0026quot;rhs\u0026quot;=\u0026quot;name\u0026quot;)) ) combined_edge_labels \u0026lt;- combined %\u0026gt;% mutate( est = round(est,2), p.code = ifelse(pvalue\u0026lt;.05,\u0026quot;p \u0026lt; .05\u0026quot;,\u0026quot;p \u0026gt; .05\u0026quot;), shape = ifelse(str_detect(name,\u0026quot;y\\\\d|x\\\\d\u0026quot;),\u0026quot;observed\u0026quot;,\u0026quot;latent\u0026quot;), midpoint.x = (x + xend)/2, midpoint.y = (y + yend)/2, x2 = ifelse(op==\u0026quot;~\u0026quot;,xend,x), xend2 = ifelse(op==\u0026quot;~\u0026quot;,x,xend), y2 = ifelse(op==\u0026quot;~\u0026quot;,yend,y), yend2 = ifelse(op==\u0026quot;~\u0026quot;,y,yend), rise = yend2-y2, run = x2-xend2, dist = sqrt(run^2 + rise^2) %\u0026gt;% round(2), newx = case_when(str_detect(rhs,\u0026quot;y\\\\d\u0026quot;) ~ (x2 + (xend2 - x2) * .90), str_detect(rhs,\u0026quot;x\\\\d\u0026quot;) ~ (x2 + (xend2 - x2) * .75), lhs == \u0026quot;dem65\u0026quot; \u0026amp; rhs == \u0026quot;dem60\u0026quot; ~ (x2 + (xend2 - x2) * .7), lhs == \u0026quot;dem65\u0026quot; \u0026amp; rhs == \u0026quot;ind60\u0026quot; ~ (x2 + (xend2 - x2) * .85), lhs == \u0026quot;dem60\u0026quot; \u0026amp; rhs == \u0026quot;ind60\u0026quot; ~ (x2 + (xend2 - x2) * .85)), newy = case_when(str_detect(rhs,\u0026quot;y\\\\d\u0026quot;) ~ (y2 + (yend2 - y2) * .90), str_detect(rhs,\u0026quot;x\\\\d\u0026quot;) ~ (y2 + (yend2 - y2) * .85), lhs == \u0026quot;dem65\u0026quot; \u0026amp; rhs == \u0026quot;dem60\u0026quot; ~ (y2 + (yend2 - y2) * .85), lhs == \u0026quot;dem65\u0026quot; \u0026amp; rhs == \u0026quot;ind60\u0026quot; ~ (y2 + (yend2 - y2) * .9), lhs == \u0026quot;dem60\u0026quot; \u0026amp; rhs == \u0026quot;ind60\u0026quot; ~ (y2 + (yend2 - y2) * .9)), )  Make the Diagram Now we plot:\ncombined_edge_labels %\u0026gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(aes(x = x2, y = y2, xend = newx, yend = newy), arrow = arrow(length = unit(6, \u0026quot;pt\u0026quot;), type = \u0026quot;closed\u0026quot;,ends = \u0026quot;last\u0026quot;)) + geom_nodes(aes(shape = factor(shape,levels = c(\u0026quot;observed\u0026quot;,\u0026quot;latent\u0026quot;))), color = \u0026quot;grey50\u0026quot;,size = 16) + geom_nodetext(aes(label = name),fontface = \u0026quot;bold\u0026quot;) + geom_label(aes(x = midpoint.x, y = midpoint.y, label = est), color = \u0026quot;black\u0026quot;,label.size = NA,hjust = .5,vjust=.5) + scale_y_continuous(expand = c(.05,0)) + scale_shape_manual(values = c(15,19),guide=F) + theme_blank()  "
 },
 {
   "uri": "/posts/dplyr/",
   "title": "dplyr verbs",
   "tags": ["R", "dplyr", "code"],
   "description": "A quick introduction dplyr&#39;s single table verbs, summarizing and grouping, and two table verbs",
   "content": "Why dplyr? The different tasks you can do with dplyr can be done with base R and other r-packages. So why learn dplyr? In my opinion, you should learn dplyr because it\nIs easy to use and understand Is fast and efficient Simplifies data manipulation Fits within broader philosophy of data science (e.g. the tidyverse)  dplyr, along with other packages from the tidyverse, were designed to work well together because they share a common “grammar” and philosophy. The most important principle of dplyr is that all functions within the package take a data.frame as input and return a data.frame as output. This simple consistency makes it possible to reason about what different functions might be doing with data. More importantly, it means that once you learn one function, you can learn other functions with relative ease.\nI would encourage you to read R for Data Science if you want to dive deeper in to the philosophy of data science from the tidyverse perspective.\n dplyr Verbs Below, you will find 3 topics that I think are the most important aspects of dplyr:\n Single-table verbs Summary and grouping functions Two-table verbs  Single table verbs will help you slice and dice and create new variables in your data. I’ve focused on the 5 most common and most widely-used single-table verbs. Summary verbs help you create useful summaries of your data quickly and help you make these summaries according to groups. Two-table verbs help you systematically merge two datasets and make it clear what the result of your merge will look like.\nThe functions described below are the main workhorse functions in dplyr but there are many others. I would encourage you to visit the dplyr website to see more tutorials and a complete function reference.\n Setup \u0026amp; Example Data Below is the code needed to set up our R session. We’ll need dplyr from the tidyverse package (you can also do library(dplyr)). I prefer to load the tidyverse package because it automatically loads a number of useful packages in a single line of code. We also need the psych package in order to grab the data we need. If you don’t have it run install.packages(\u0026quot;psych\u0026quot;).\n# Load packages library(tidyverse) library(psych) # Datasets from the psych package data(\u0026quot;bfi\u0026quot;) data(\u0026quot;bfi.dictionary\u0026quot;) # Convert datasets to tibbles bfi \u0026lt;- as_tibble(bfi) This dataset is a sample of 2,800 observations for which 25 personality self report items were collected from the International Personality Item Pool (IPIP) as a part of the SAPA (see the psych package description of for more details).\nTo get a sense for these data, we can use glimpse() from dplyr to print out the dimensions of the dataset, the variables and their types, and the first few observations of each variable.\nbfi data glimpse(bfi) ## Observations: 2,800 ## Variables: 28 ## $ A1 \u0026lt;int\u0026gt; 2, 2, 5, 4, 2, 6, 2, 4, 4, 2, 4, 2, 5, 5, 4, 4, 4, 5, … ## $ A2 \u0026lt;int\u0026gt; 4, 4, 4, 4, 3, 6, 5, 3, 3, 5, 4, 5, 5, 5, 5, 3, 6, 5, … ## $ A3 \u0026lt;int\u0026gt; 3, 5, 5, 6, 3, 5, 5, 1, 6, 6, 5, 5, 5, 5, 2, 6, 6, 5, … ## $ A4 \u0026lt;int\u0026gt; 4, 2, 4, 5, 4, 6, 3, 5, 3, 6, 6, 5, 6, 6, 2, 6, 2, 4, … ## $ A5 \u0026lt;int\u0026gt; 4, 5, 4, 5, 5, 5, 5, 1, 3, 5, 5, 5, 4, 6, 1, 3, 5, 5, … ## $ C1 \u0026lt;int\u0026gt; 2, 5, 4, 4, 4, 6, 5, 3, 6, 6, 4, 5, 5, 4, 5, 5, 4, 5, … ## $ C2 \u0026lt;int\u0026gt; 3, 4, 5, 4, 4, 6, 4, 2, 6, 5, 3, 4, 4, 4, 5, 5, 4, 5, … ## $ C3 \u0026lt;int\u0026gt; 3, 4, 4, 3, 5, 6, 4, 4, 3, 6, 5, 5, 3, 4, 5, 5, 4, 5, … ## $ C4 \u0026lt;int\u0026gt; 4, 3, 2, 5, 3, 1, 2, 2, 4, 2, 3, 4, 2, 2, 2, 3, 4, 4, … ## $ C5 \u0026lt;int\u0026gt; 4, 4, 5, 5, 2, 3, 3, 4, 5, 1, 2, 5, 2, 1, 2, 5, 4, 3, … ## $ E1 \u0026lt;int\u0026gt; 3, 1, 2, 5, 2, 2, 4, 3, 5, 2, 1, 3, 3, 2, 3, 1, 1, 2, … ## $ E2 \u0026lt;int\u0026gt; 3, 1, 4, 3, 2, 1, 3, 6, 3, 2, 3, 3, 3, 2, 4, 1, 2, 2, … ## $ E3 \u0026lt;int\u0026gt; 3, 6, 4, 4, 5, 6, 4, 4, NA, 4, 2, 4, 3, 4, 3, 6, 5, 4,… ## $ E4 \u0026lt;int\u0026gt; 4, 4, 4, 4, 4, 5, 5, 2, 4, 5, 5, 5, 2, 6, 6, 6, 5, 6, … ## $ E5 \u0026lt;int\u0026gt; 4, 3, 5, 4, 5, 6, 5, 1, 3, 5, 4, 4, 4, 5, 5, 4, 5, 6, … ## $ N1 \u0026lt;int\u0026gt; 3, 3, 4, 2, 2, 3, 1, 6, 5, 5, 3, 4, 1, 1, 2, 4, 4, 6, … ## $ N2 \u0026lt;int\u0026gt; 4, 3, 5, 5, 3, 5, 2, 3, 5, 5, 3, 5, 2, 1, 4, 5, 4, 5, … ## $ N3 \u0026lt;int\u0026gt; 2, 3, 4, 2, 4, 2, 2, 2, 2, 5, 4, 3, 2, 1, 2, 4, 4, 5, … ## $ N4 \u0026lt;int\u0026gt; 2, 5, 2, 4, 4, 2, 1, 6, 3, 2, 2, 2, 2, 2, 2, 5, 4, 4, … ## $ N5 \u0026lt;int\u0026gt; 3, 5, 3, 1, 3, 3, 1, 4, 3, 4, 3, NA, 2, 1, 3, 5, 5, 4,… ## $ O1 \u0026lt;int\u0026gt; 3, 4, 4, 3, 3, 4, 5, 3, 6, 5, 5, 4, 4, 5, 5, 6, 5, 5, … ## $ O2 \u0026lt;int\u0026gt; 6, 2, 2, 3, 3, 3, 2, 2, 6, 1, 3, 6, 2, 3, 2, 6, 1, 1, … ## $ O3 \u0026lt;int\u0026gt; 3, 4, 5, 4, 4, 5, 5, 4, 6, 5, 5, 4, 4, 4, 5, 6, 5, 4, … ## $ O4 \u0026lt;int\u0026gt; 4, 3, 5, 3, 3, 6, 6, 5, 6, 5, 6, 5, 5, 4, 5, 3, 6, 5, … ## $ O5 \u0026lt;int\u0026gt; 3, 3, 2, 5, 3, 1, 1, 3, 1, 2, 3, 4, 2, 4, 5, 2, 3, 4, … ## $ gender \u0026lt;int\u0026gt; 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, … ## $ education \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, 3, NA, 2, 1, NA, 1, NA, NA, NA, 1,… ## $ age \u0026lt;int\u0026gt; 16, 18, 17, 17, 17, 21, 18, 19, 19, 17, 21, 16, 16, 16…   Single Table Verbs The first set of dplyr verbs that we will talk about and use are single-table verbs. Single-table operations are the most common and widely used verbs used in data manipulation. When I say “data manipulation”, I am referring to:\n selecting the relevant columns from a larger dataset (i.e. variables) renaming variables with more useful labels filtering the relevant rows (i.e. observations or cases) to the ones you want to analyze arranging or sorting the observations in ways that help you inspect your data creating new variables based on existing variables (e.g. creating scale scores from a set of items).  These operations are likely very familiar to you but, at least in my beginning experiences with R, it was not always clear how they were executed in R. On top of this issue, it wasn’t clear to me that this process was or could be systematic. dplyr makes these operations more explicit and helps you think about how to do such operations systematically. In fact, the so-called 5 most important verbs of dplyrdo exactly what they sound like:\n  function  description      select()  Select relevant columns of your data    rename()  Rename the columns of your data    filter()  Filter your data according to logical statements    arrange()  Sort your data on a certain column, ascending or descending    mutate()  Create new variables and add them to your dataset     Below are some examples of ways that I use these verbs in my work:\n Select  Selecting variables is probably one of the most powerful dplyr operations. All that you need to do in order to select variables in a dataset is simply write out thier names (unquoted), like so: select(data,var1,var2,var3). This code will select var1,var2, and var3 from the dataset data and give you a new dataset that only contains those columns.\nYou can also “deselect” columns. For example, let’s say you didn’t want var1,var2, and var3 in your data but you wanted to keep everything else. Simply write the following code: select(data,-var1,-var2,-var3). The - will drop those columns and give all other columns in your data.\nYou can also do more complicated things (see below). For example, you can select all columns that have a particular prefix, suffix, or contain a particular word or certain letter sequences. Take a look at the example:\nExample First, because I am not terribly familiar with the BFI dataset from the psych package, I want to figure out which variables I actually need to look at personality. Luckily, the psych package provides a bfi.dictionary for this exact purpose.\nbfi.dictionary %\u0026gt;% rownames_to_column() %\u0026gt;% rename(bfi_item = rowname) %\u0026gt;% as_tibble() ## # A tibble: 28 x 8 ## bfi_item ItemLabel Item Giant3 Big6 Little12 Keying IPIP100 ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; ## 1 A1 q_146 Am indiffer… Cohesi… Agreea… Compassi… -1 B5:A ## 2 A2 q_1162 Inquire abo… Cohesi… Agreea… Compassi… 1 B5:A ## 3 A3 q_1206 Know how to… Cohesi… Agreea… Compassi… 1 B5:A ## 4 A4 q_1364 Love childr… Cohesi… Agreea… Compassi… 1 B5:A ## 5 A5 q_1419 Make people… Cohesi… Agreea… Compassi… 1 B5:A ## 6 C1 q_124 Am exacting… Stabil… Consci… Orderlin… 1 B5:C ## 7 C2 q_530 Continue un… Stabil… Consci… Orderlin… 1 B5:C ## 8 C3 q_619 Do things a… Stabil… Consci… Orderlin… 1 B5:C ## 9 C4 q_626 Do things i… Stabil… Consci… Industri… -1 B5:C ## 10 C5 q_1949 Waste my ti… Stabil… Consci… Industri… -1 B5:C ## # … with 18 more rows After searching throught the codebook, I was able to deduce that all the personality variables have capital letter prefix for the trait that they measure with a trailing digit indicating the item number. Now I can quickly select subsets of these items depending on my needs.\nSelect by variable name Let’s say I just want to select a couple variables. This is the most straightforward way to use select. For example, I can select gender, age, and the 5 items that measure Agreeableness like so:\n# Spell out the variables you want to select bfi %\u0026gt;% select(gender, age, A1, A2, A3, A4, A5) ## # A tibble: 2,800 x 7 ## gender age A1 A2 A3 A4 A5 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 16 2 4 3 4 4 ## 2 2 18 2 4 5 2 5 ## 3 2 17 5 4 5 4 4 ## 4 2 17 4 4 6 5 5 ## 5 1 17 2 3 3 4 5 ## 6 2 21 6 6 5 6 5 ## 7 1 18 2 5 5 3 5 ## 8 1 19 4 3 1 5 1 ## 9 1 19 4 3 6 3 3 ## 10 2 17 2 5 6 6 5 ## # … with 2,790 more rows Alternatively, let’s say I want everything but gender, age, and education:\n# select everything but gender, age, and education bfi %\u0026gt;% select(-gender, -age, -education)  ## # A tibble: 2,800 x 25 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2 4 3 4 4 2 3 3 4 4 3 3 ## 2 2 4 5 2 5 5 4 4 3 4 1 1 ## 3 5 4 5 4 4 4 5 4 2 5 2 4 ## 4 4 4 6 5 5 4 4 3 5 5 5 3 ## 5 2 3 3 4 5 4 4 5 3 2 2 2 ## 6 6 6 5 6 5 6 6 6 1 3 2 1 ## 7 2 5 5 3 5 5 4 4 2 3 4 3 ## 8 4 3 1 5 1 3 2 4 2 4 3 6 ## 9 4 3 6 3 3 6 6 3 4 5 5 3 ## 10 2 5 6 6 5 6 5 6 2 1 2 2 ## # … with 2,790 more rows, and 13 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt; Or, what if you know you just want the first few columns of the dataset and you don’t want to type their names?\n# select the first 5 colunmns bfi %\u0026gt;% select(1:5)  ## # A tibble: 2,800 x 5 ## A1 A2 A3 A4 A5 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2 4 3 4 4 ## 2 2 4 5 2 5 ## 3 5 4 5 4 4 ## 4 4 4 6 5 5 ## 5 2 3 3 4 5 ## 6 6 6 5 6 5 ## 7 2 5 5 3 5 ## 8 4 3 1 5 1 ## 9 4 3 6 3 3 ## 10 2 5 6 6 5 ## # … with 2,790 more rows Select by string matches Using select with variable names is powerful but can involve a lot of typing if you need to select many variables. An even more powerful way to select is to utilize “select helpers”. These include (descriptions from select_helpers help page):\n starts_with(): Starts with a prefix. ends_with(): Ends with a suffix. contains(): Contains a literal string. num_range(): Matches a numerical range like x01, x02, x03. one_of(): Matches variable names in a character vector. matches(): Matches a regular expression. everything(): Matches all variables. last_col(): Select last variable, possibly with an offset.  I use starts_with(), ends_with(), and contains() most frequently. However, matches() is the most powerful as it allows you to leverage regular expressions. A full discussion of regular experssions is beyond the scope of this post. In brief, regular expressions allow you to do complex string pattern matching.\nExamples For the following examples, I will be printing out the results of using different select_helpers. Note that I am going to print out column names (with names()) rather than the whole dataset for brevity.\nstarts_with(): Let’s say we only want Conscientiousness items:\nbfi %\u0026gt;% select(starts_with(\u0026quot;C\u0026quot;)) %\u0026gt;% names() ## [1] \u0026quot;C1\u0026quot; \u0026quot;C2\u0026quot; \u0026quot;C3\u0026quot; \u0026quot;C4\u0026quot; \u0026quot;C5\u0026quot; ends_with(): how about the first item of each scale?:\nbfi %\u0026gt;% select(ends_with(\u0026quot;1\u0026quot;)) %\u0026gt;% names() ## [1] \u0026quot;A1\u0026quot; \u0026quot;C1\u0026quot; \u0026quot;E1\u0026quot; \u0026quot;N1\u0026quot; \u0026quot;O1\u0026quot; contains(): how about all the Openness items?:\nbfi %\u0026gt;% select(contains(\u0026quot;O\u0026quot;)) %\u0026gt;% names() ## [1] \u0026quot;O1\u0026quot; \u0026quot;O2\u0026quot; \u0026quot;O3\u0026quot; \u0026quot;O4\u0026quot; \u0026quot;O5\u0026quot; \u0026quot;education\u0026quot; Note that in this case contains() wasn’t great because education contains an “o”. We can fix that by specifying we want a capitol “O”.\nbfi %\u0026gt;% select(contains(\u0026quot;O\u0026quot;, ignore.case = F)) %\u0026gt;% names() ## [1] \u0026quot;O1\u0026quot; \u0026quot;O2\u0026quot; \u0026quot;O3\u0026quot; \u0026quot;O4\u0026quot; \u0026quot;O5\u0026quot; num_range(): how about the last three items of Emotional Stability?:\nbfi %\u0026gt;% select(num_range(prefix = \u0026quot;N\u0026quot;, 3:5)) %\u0026gt;% names() ## [1] \u0026quot;N3\u0026quot; \u0026quot;N4\u0026quot; \u0026quot;N5\u0026quot; one_of(): Let’s say you wanted the last item of Emotional Stabilty but you weren’t sure if there is 5 or 6 items:\nbfi %\u0026gt;% select(one_of(\u0026quot;N5\u0026quot;,\u0026quot;N6\u0026quot;)) %\u0026gt;% names() ## Warning: Unknown columns: `N6` ## [1] \u0026quot;N5\u0026quot; matches(): Let’s do something more complicated. How about finding all of the Conscientiousness items, the last 2 items of Emotional Stability, and the first three items of Openness?\nbfi %\u0026gt;% select(matches(\u0026quot;^O[1-3]|^N[4-5]|^C\u0026quot;)) %\u0026gt;% names() ## [1] \u0026quot;C1\u0026quot; \u0026quot;C2\u0026quot; \u0026quot;C3\u0026quot; \u0026quot;C4\u0026quot; \u0026quot;C5\u0026quot; \u0026quot;N4\u0026quot; \u0026quot;N5\u0026quot; \u0026quot;O1\u0026quot; \u0026quot;O2\u0026quot; \u0026quot;O3\u0026quot;   Rename  Renaming variables is pretty simply in dplyr. Simply type rename(data, new.name.1 = old.name.1,new.name.2 = old.name.2). Here, you provide a new name (e.g. new.name.1) and set it equal to the old name in the data (e.g. old.name.1). See the example to see how rename() works.\nNote: that the more complicated examples are there to inspire you to learn how to systematically and programatically change many variables all at once.\nExamples # Old names names(bfi) ## [1] \u0026quot;A1\u0026quot; \u0026quot;A2\u0026quot; \u0026quot;A3\u0026quot; \u0026quot;A4\u0026quot; \u0026quot;A5\u0026quot; ## [6] \u0026quot;C1\u0026quot; \u0026quot;C2\u0026quot; \u0026quot;C3\u0026quot; \u0026quot;C4\u0026quot; \u0026quot;C5\u0026quot; ## [11] \u0026quot;E1\u0026quot; \u0026quot;E2\u0026quot; \u0026quot;E3\u0026quot; \u0026quot;E4\u0026quot; \u0026quot;E5\u0026quot; ## [16] \u0026quot;N1\u0026quot; \u0026quot;N2\u0026quot; \u0026quot;N3\u0026quot; \u0026quot;N4\u0026quot; \u0026quot;N5\u0026quot; ## [21] \u0026quot;O1\u0026quot; \u0026quot;O2\u0026quot; \u0026quot;O3\u0026quot; \u0026quot;O4\u0026quot; \u0026quot;O5\u0026quot; ## [26] \u0026quot;gender\u0026quot; \u0026quot;education\u0026quot; \u0026quot;age\u0026quot; Rename just a couple of variables:\n# Simple rename bfi %\u0026gt;% rename(Agreeableness_1 = A1, Agreeableness_2 = A2) %\u0026gt;% names() ## [1] \u0026quot;Agreeableness_1\u0026quot; \u0026quot;Agreeableness_2\u0026quot; \u0026quot;A3\u0026quot; ## [4] \u0026quot;A4\u0026quot; \u0026quot;A5\u0026quot; \u0026quot;C1\u0026quot; ## [7] \u0026quot;C2\u0026quot; \u0026quot;C3\u0026quot; \u0026quot;C4\u0026quot; ## [10] \u0026quot;C5\u0026quot; \u0026quot;E1\u0026quot; \u0026quot;E2\u0026quot; ## [13] \u0026quot;E3\u0026quot; \u0026quot;E4\u0026quot; \u0026quot;E5\u0026quot; ## [16] \u0026quot;N1\u0026quot; \u0026quot;N2\u0026quot; \u0026quot;N3\u0026quot; ## [19] \u0026quot;N4\u0026quot; \u0026quot;N5\u0026quot; \u0026quot;O1\u0026quot; ## [22] \u0026quot;O2\u0026quot; \u0026quot;O3\u0026quot; \u0026quot;O4\u0026quot; ## [25] \u0026quot;O5\u0026quot; \u0026quot;gender\u0026quot; \u0026quot;education\u0026quot; ## [28] \u0026quot;age\u0026quot; Rename a set of variables using a rename_at() (combination of select and rename):\n# More complicated rename bfi %\u0026gt;% rename_at(vars(matches(\u0026quot;^A\\\\d\u0026quot;)), funs(paste0(\u0026quot;Agreeableness_\u0026quot;,1:5))) %\u0026gt;% names() ## [1] \u0026quot;Agreeableness_1\u0026quot; \u0026quot;Agreeableness_2\u0026quot; \u0026quot;Agreeableness_3\u0026quot; ## [4] \u0026quot;Agreeableness_4\u0026quot; \u0026quot;Agreeableness_5\u0026quot; \u0026quot;C1\u0026quot; ## [7] \u0026quot;C2\u0026quot; \u0026quot;C3\u0026quot; \u0026quot;C4\u0026quot; ## [10] \u0026quot;C5\u0026quot; \u0026quot;E1\u0026quot; \u0026quot;E2\u0026quot; ## [13] \u0026quot;E3\u0026quot; \u0026quot;E4\u0026quot; \u0026quot;E5\u0026quot; ## [16] \u0026quot;N1\u0026quot; \u0026quot;N2\u0026quot; \u0026quot;N3\u0026quot; ## [19] \u0026quot;N4\u0026quot; \u0026quot;N5\u0026quot; \u0026quot;O1\u0026quot; ## [22] \u0026quot;O2\u0026quot; \u0026quot;O3\u0026quot; \u0026quot;O4\u0026quot; ## [25] \u0026quot;O5\u0026quot; \u0026quot;gender\u0026quot; \u0026quot;education\u0026quot; ## [28] \u0026quot;age\u0026quot; labels \u0026lt;- bfi.dictionary %\u0026gt;% filter(str_detect(ItemLabel,\u0026quot;\\\\d$\u0026quot;)) %\u0026gt;% pull(Big6) %\u0026gt;% as.character # Even more complicated rename bfi %\u0026gt;% rename_at(vars(matches(\u0026quot;\\\\d$\u0026quot;)), funs(str_replace(.,\u0026quot;^[[:upper:]]\u0026quot;,labels))) %\u0026gt;% names() ## [1] \u0026quot;Agreeableness1\u0026quot; \u0026quot;Agreeableness2\u0026quot; \u0026quot;Agreeableness3\u0026quot; ## [4] \u0026quot;Agreeableness4\u0026quot; \u0026quot;Agreeableness5\u0026quot; \u0026quot;Conscientiousness1\u0026quot; ## [7] \u0026quot;Conscientiousness2\u0026quot; \u0026quot;Conscientiousness3\u0026quot; \u0026quot;Conscientiousness4\u0026quot; ## [10] \u0026quot;Conscientiousness5\u0026quot; \u0026quot;Extraversion1\u0026quot; \u0026quot;Extraversion2\u0026quot; ## [13] \u0026quot;Extraversion3\u0026quot; \u0026quot;Extraversion4\u0026quot; \u0026quot;Extraversion5\u0026quot; ## [16] \u0026quot;Emotional Stability1\u0026quot; \u0026quot;Emotional Stability2\u0026quot; \u0026quot;Emotional Stability3\u0026quot; ## [19] \u0026quot;Emotional Stability4\u0026quot; \u0026quot;Emotional Stability5\u0026quot; \u0026quot;Openness1\u0026quot; ## [22] \u0026quot;Openness2\u0026quot; \u0026quot;Openness3\u0026quot; \u0026quot;Openness4\u0026quot; ## [25] \u0026quot;Openness5\u0026quot; \u0026quot;gender\u0026quot; \u0026quot;education\u0026quot; ## [28] \u0026quot;age\u0026quot;   Arrange  Arranging columns is also very straight forward. Simply indicate which variable you want to use to arrange the data: arrange(data,column.to.arrange.by). You can specify a column wrapped in desc() to have it ordered in descending order instead.\nExample bfi %\u0026gt;% arrange(age) ## # A tibble: 2,800 x 28 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2 5 5 4 5 5 3 5 2 4 2 5 ## 2 1 4 5 NA 5 4 1 6 5 3 1 1 ## 3 1 6 6 6 6 5 6 5 1 1 1 1 ## 4 1 6 6 6 5 2 5 5 5 2 1 1 ## 5 NA 6 4 NA 4 4 NA 6 NA 1 1 5 ## 6 1 6 6 5 2 4 2 2 5 4 6 1 ## 7 4 4 2 4 4 4 4 4 3 4 3 5 ## 8 2 5 3 2 2 4 2 5 5 5 6 6 ## 9 1 6 6 6 6 6 6 6 1 1 1 1 ## 10 4 5 5 4 2 2 2 4 3 1 5 5 ## # … with 2,790 more rows, and 16 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt; bfi %\u0026gt;% arrange(desc(age)) ## # A tibble: 2,800 x 28 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 3 4 4 4 4 1 5 3 2 6 2 ## 2 4 3 1 1 5 6 6 4 1 1 6 2 ## 3 1 6 6 4 6 2 4 4 1 2 2 1 ## 4 2 4 5 6 6 5 3 5 2 2 2 4 ## 5 1 5 6 5 6 4 3 2 4 5 2 1 ## 6 2 4 4 3 4 3 2 3 4 6 6 5 ## 7 2 4 4 4 4 5 5 4 1 2 4 4 ## 8 1 4 3 6 5 5 4 2 4 5 5 5 ## 9 2 6 6 6 5 5 5 5 1 3 2 2 ## 10 5 4 2 6 4 5 6 4 2 5 1 6 ## # … with 2,790 more rows, and 16 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt;   Filter  Filtering data is an operation that you will undoubtedly need to use all the time. You filter data anytime you need to create some subset of a larger data set. To perform this operation you need to supply filter() with a logical expression. This expression will be applied to the dataset and only rows that meet your criteria (i.e. evaluate to TRUE after your logical expression), will be kept. Take a look at the example:\nExample This dataset is pretty big (N = 2800). I might want to use everyone in this dataset but it’s reasonable to see how certain research questions may not require the entire sample. For example, maybe I only want to look at adults who are younger than 65. This could be because 65 and younger adults are likely not retired, or maybe after 40 is a meaningful cutoff for certain questions.\nWhatever the case, you can quickly subset your data using filter(). Below I use the expression Age \u0026lt; 40 inside my call to filter(). This expression will help filter() figure out which individuals are younger than 65 and only keep those individuals.\nbfi %\u0026gt;% filter(age \u0026lt;= 40) %\u0026gt;% select(age,everything()) %\u0026gt;% arrange(desc(age)) ## # A tibble: 2,358 x 28 ## age A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 40 1 5 5 6 5 4 4 4 3 4 4 ## 2 40 1 5 5 5 2 6 NA 6 1 1 3 ## 3 40 1 5 NA 5 6 6 6 1 1 1 6 ## 4 40 1 6 4 6 6 5 4 5 1 2 5 ## 5 40 4 4 4 NA 5 4 4 4 NA 3 4 ## 6 40 1 5 5 6 5 5 4 4 4 4 1 ## 7 40 3 5 5 6 5 5 4 5 2 3 2 ## 8 40 3 6 5 6 6 5 6 5 1 3 1 ## 9 40 1 6 6 6 6 1 5 6 1 1 1 ## 10 40 1 6 4 6 6 5 6 4 3 4 3 ## # … with 2,348 more rows, and 16 more variables: E2 \u0026lt;int\u0026gt;, E3 \u0026lt;int\u0026gt;, ## # E4 \u0026lt;int\u0026gt;, E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, ## # O1 \u0026lt;int\u0026gt;, O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, ## # education \u0026lt;int\u0026gt;   Mutate  Mutate is the final (mainstream) verb among the single-table dplyr verbs. It’s a little more complicated than the others but I still think it’s intuitive. The point of mutate() is to create new variables based on existing variables and add them to your data.\nTo use mutate() simply give your new variable a name followed by an =. Then, express how you want to calculate your new variable. See below for examples:\nExample To see how mutate() works, let’s create composite scores for each personality trait.\nbfi %\u0026gt;% rowwise() %\u0026gt;% # make sure to calculate means across rows not columns mutate( Neuroticism = mean(c(A1,A2,A3,A4,A5),na.rm=T), Extraversion = mean(c(C1,C2,C3,C4,C5),na.rm=T), Openness = mean(c(E1,E2,E3,E4,E5),na.rm=T), Conscientiousness = mean(c(N1,N2,N3,N4,N5),na.rm=T), Agreeableness = mean(c(O1,O2,O3,O4,O5),na.rm=T) ) %\u0026gt;% select(Neuroticism, Extraversion, Openness, Conscientiousness, Agreeableness) ## Source: local data frame [2,800 x 5] ## Groups: \u0026lt;by row\u0026gt; ## ## # A tibble: 2,800 x 5 ## Neuroticism Extraversion Openness Conscientiousness Agreeableness ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3.4 3.2 3.4 2.8 3.8 ## 2 3.6 4 3 3.8 3.2 ## 3 4.4 4 3.8 3.6 3.6 ## 4 4.8 4.2 4 2.8 3.6 ## 5 3.4 3.6 3.6 3.2 3.2 ## 6 5.6 4.4 4 3 3.8 ## 7 4 3.6 4.2 1.4 3.8 ## 8 2.8 3 3.2 4.2 3.4 ## 9 3.8 4.8 3.75 3.6 5 ## 10 4.8 4 3.6 4.2 3.6 ## # … with 2,790 more rows   Putting it all together  Now that you have been introduced to the most important single-table dplyr verbs, let’s see how we might complete all of these steps in a single chain of function calls:\nbfi %\u0026gt;% filter(age \u0026lt;= 40) %\u0026gt;% rowwise() %\u0026gt;% mutate(Neuroticism = mean(c(A1,A2,A3,A4,A5),na.rm=T), Extraversion = mean(c(C1,C2,C3,C4,C5),na.rm=T), Openness = mean(c(E1,E2,E3,E4,E5),na.rm=T), Conscientiousness = mean(c(N1,N2,N3,N4,N5),na.rm=T), Agreeableness = mean(c(O1,O2,O3,O4,O5),na.rm=T)) %\u0026gt;% select(age,education,gender,Neuroticism, Extraversion, Openness, Conscientiousness, Agreeableness) %\u0026gt;% rename_all(tolower) %\u0026gt;% arrange(desc(age)) ## # A tibble: 2,358 x 8 ## age education gender neuroticism extraversion openness ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 40 3 2 4.4 3.8 3.8 ## 2 40 5 2 3.6 3.5 2.8 ## 3 40 3 2 4.25 3 2.8 ## 4 40 3 1 4.6 3.4 4.2 ## 5 40 5 2 4.25 3.75 3.75 ## 6 40 1 2 4.4 4.2 3.4 ## 7 40 3 2 4.8 3.8 3.8 ## 8 40 2 2 5.2 4 4 ## 9 40 3 2 5 2.8 3.6 ## 10 40 3 2 4.6 4.4 3.6 ## # … with 2,348 more rows, and 2 more variables: conscientiousness \u0026lt;dbl\u0026gt;, ## # agreeableness \u0026lt;dbl\u0026gt;   \n Summarizing and Grouping Summarizing data can be tedious. It involves taking raw data and turning those data into useful summary statistics (e.g. means, standard deviations, minimum and maximun values, ranges, etc.). Furthermore, it’s often useful to create such summaries within subgroups. For example, you may want to create summary values for each condition of an experiment or some other grouping variable.\ndplyr has a set of functions that specifically handle these operations and make it very easy and systematic to create the summaries you want to create.\n Summarize  Summarizing in dplyr works the same way as mutate(). Using the function summarize(), we can specify a data set we want to summarize, give the name of the summary variable we want to create, and then a specific operation to perform. For example, if we wanted to find the mean of a single variable in a dataset we might write summarize(data, summary.variable = mean(var1)). The result of this function will be a single value: the mean of var1.\nExample - Simple Summaries Here, I want to know, for the whole dataset, what the mean, median, standard deviation, minimun and maximum ages in the BFI dataset.\nbfi %\u0026gt;% summarize( mean = mean(age,na.rm=T), median = median(age,na.rm=T), sd = sd(age,na.rm=T), min = min(age,na.rm=T), max = max(age,na.rm=T) ) ## # A tibble: 1 x 5 ## mean median sd min max ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 28.8 26 11.1 3 86   Group By  The power of summarize() becomes much greater when you use it in conjunction with group_by(). The point of group_by() is to group data into categories and perform operations on them. For example, maybe we want to know the mean of a particular variable but within a particular group category. We might right group_by(data, grouping.variable) %\u0026gt;% summarize(mean = mean(var1)). This will become more clear in the example below:\nExample - Grouped Summaries To see the utility of group_by() and summarize() let’s suppose we wanted to know all the same summary statistics for age but within each education level We could simply add one line to our already written code to make this happen seemlessly:\nbfi %\u0026gt;% group_by(education) %\u0026gt;% summarize( mean = mean(age,na.rm=T), median = median(age,na.rm=T), sd = sd(age,na.rm=T), min = min(age,na.rm=T), max = max(age,na.rm=T) ) ## # A tibble: 6 x 6 ## education mean median sd min max ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 25.1 20 10.4 14 62 ## 2 2 31.5 27 12.2 17 86 ## 3 3 27.2 24 9.45 11 63 ## 4 4 33.0 30 10.3 18 70 ## 5 5 35.3 32 11.0 3 74 ## 6 NA 18.0 16 8.52 9 61 The possibilities are quite broad once you start getting used the logic of grouping and summarizing variables. For example, you can make summary variables based on multiple grouping variables. Take a look:\nbfi %\u0026gt;% group_by(education,gender) %\u0026gt;% summarize( mean = mean(age,na.rm=T), median = median(age,na.rm=T), sd = sd(age,na.rm=T), min = min(age,na.rm=T), max = max(age,na.rm=T) ) ## # A tibble: 12 x 7 ## # Groups: education [?] ## education gender mean median sd min max ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 1 25.2 20 9.82 15 53 ## 2 1 2 25.1 20 10.8 14 62 ## 3 2 1 31.5 27 12.3 18 65 ## 4 2 2 31.5 27 12.3 17 86 ## 5 3 1 25.4 22 8.13 16 60 ## 6 3 2 28.0 25 9.83 11 63 ## 7 4 1 33.2 30 10.7 20 70 ## 8 4 2 32.9 30 10.2 18 59 ## 9 5 1 33.9 30 12.0 3 74 ## 10 5 2 36.1 34 10.3 19 66 ## 11 NA 1 18.9 17 9.37 12 55 ## 12 NA 2 17.4 16 7.98 9 61   \n Two-Table Verbs Two-table verbs are dplyr functions that use two datasets and do something with them. Most commonly, these two-table verbs are used to merge data. However, as we will see, merging data is not necessarily a simple task and many problems arise when attempting even the simpliest of merges.\nIn general, there are two types of joins:\nMutating joins, ones that add more variables to your data Filtering joings, ones that operate only on the observations of the data and do not add any new variables to your data.  When using join functions, you will be explicitly supplying 2 data.frames and specific columns to match by. For example, you might want to merge two datasets from different timepoints in a longitudinal study. Thus, you will merge these datasets using a key, such as participant ID.\nExample Data To show how two-table verbs work, I will need another dataset to merge with the BFI data. The dataset we will use is a sample of 1,525 subjects from the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project (see the psych pacakge description for more details). The dataset contains variables that measure cognitive performance. Below is the code I used to get these data into R:\n# load the data from psych package data(\u0026quot;ability\u0026quot;) # convert it to a tibble ability \u0026lt;- as_tibble(ability) # take a look at the variables glimpse(ability) ## Observations: 1,525 ## Variables: 16 ## $ reason.4 \u0026lt;dbl\u0026gt; 0, 0, 0, 1, NA, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,… ## $ reason.16 \u0026lt;dbl\u0026gt; 0, 0, 1, NA, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,… ## $ reason.17 \u0026lt;dbl\u0026gt; 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, … ## $ reason.19 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, … ## $ letter.7 \u0026lt;dbl\u0026gt; 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, NA, 1, 0, 1, 0, 1, 1, 1,… ## $ letter.33 \u0026lt;dbl\u0026gt; 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, … ## $ letter.34 \u0026lt;dbl\u0026gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, … ## $ letter.58 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, NA, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,… ## $ matrix.45 \u0026lt;dbl\u0026gt; 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, … ## $ matrix.46 \u0026lt;dbl\u0026gt; 0, 0, 1, NA, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,… ## $ matrix.47 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, … ## $ matrix.55 \u0026lt;dbl\u0026gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, … ## $ rotate.3 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, … ## $ rotate.4 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 1, 1, 1, 0, NA, 0, 0, 0, 0, 0, 0, 1, 1,… ## $ rotate.6 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, … ## $ rotate.8 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …  Matching values The whole idea of joining datasets is predicated on the assumption that tables contain at least some of the same observations. In our case, we want to join the BFI data with the ability data so that we can look at participants who have completed personality and ability items. Our datasets do not actually have the same observations or at there is no column of unique observation identifiers to help us join the tables. As such, we are going to make some observation identifiers ourselves.\n# Set seed so you get the same results as me set.seed(1) # make ID numbers for the 2800 observations in the BFI data bfi_fake_ids \u0026lt;- bfi %\u0026gt;% mutate(id = 1:n()) # make ID numbers for the 1525 observations in the ability data based on the BFI IDs ability_fake_ids \u0026lt;- ability %\u0026gt;% mutate(id = c(sample(bfi_fake_ids$id,1000,replace = F),3001:3525)) # make some IDs from bfi and some new ones  Mutating Joins Remember, mutating joins merge together two datasets. They are ‘mutating’ because the resulting merged dataset will contain more variables.\n Inner Join  Inner joins (using inner_join()) will always return a data set that contains observations that exist in both data sets. As such, if I do an inner_join() using the BFI and ability data, the newly joined dataset should only contain observations that match based on ID numbers:\ninner_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) ## # A tibble: 1,000 x 45 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 5 4 5 4 4 4 5 4 2 5 2 4 ## 2 2 3 3 4 5 4 4 5 3 2 2 2 ## 3 2 5 6 6 5 6 5 6 2 1 2 2 ## 4 4 4 5 4 3 5 4 5 4 6 1 2 ## 5 1 6 6 1 5 5 4 4 2 3 1 2 ## 6 4 5 5 6 5 5 5 4 1 1 3 2 ## 7 1 6 6 1 6 5 2 5 1 1 1 1 ## 8 2 4 4 4 3 6 5 6 1 1 2 4 ## 9 2 4 5 6 5 4 6 4 2 4 2 2 ## 10 5 3 5 4 2 2 2 4 3 4 3 4 ## # … with 990 more rows, and 33 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt;, id \u0026lt;int\u0026gt;, reason.4 \u0026lt;dbl\u0026gt;, reason.16 \u0026lt;dbl\u0026gt;, reason.17 \u0026lt;dbl\u0026gt;, ## # reason.19 \u0026lt;dbl\u0026gt;, letter.7 \u0026lt;dbl\u0026gt;, letter.33 \u0026lt;dbl\u0026gt;, letter.34 \u0026lt;dbl\u0026gt;, ## # letter.58 \u0026lt;dbl\u0026gt;, matrix.45 \u0026lt;dbl\u0026gt;, matrix.46 \u0026lt;dbl\u0026gt;, matrix.47 \u0026lt;dbl\u0026gt;, ## # matrix.55 \u0026lt;dbl\u0026gt;, rotate.3 \u0026lt;dbl\u0026gt;, rotate.4 \u0026lt;dbl\u0026gt;, rotate.6 \u0026lt;dbl\u0026gt;, ## # rotate.8 \u0026lt;dbl\u0026gt;   Left \u0026amp; Right Join  left_join() A left_join() keeps all observations from the data.frame on the left and grabs only the observations from the right data.frame that match the left:\nleft_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) ## # A tibble: 2,800 x 45 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2 4 3 4 4 2 3 3 4 4 3 3 ## 2 2 4 5 2 5 5 4 4 3 4 1 1 ## 3 5 4 5 4 4 4 5 4 2 5 2 4 ## 4 4 4 6 5 5 4 4 3 5 5 5 3 ## 5 2 3 3 4 5 4 4 5 3 2 2 2 ## 6 6 6 5 6 5 6 6 6 1 3 2 1 ## 7 2 5 5 3 5 5 4 4 2 3 4 3 ## 8 4 3 1 5 1 3 2 4 2 4 3 6 ## 9 4 3 6 3 3 6 6 3 4 5 5 3 ## 10 2 5 6 6 5 6 5 6 2 1 2 2 ## # … with 2,790 more rows, and 33 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt;, id \u0026lt;int\u0026gt;, reason.4 \u0026lt;dbl\u0026gt;, reason.16 \u0026lt;dbl\u0026gt;, reason.17 \u0026lt;dbl\u0026gt;, ## # reason.19 \u0026lt;dbl\u0026gt;, letter.7 \u0026lt;dbl\u0026gt;, letter.33 \u0026lt;dbl\u0026gt;, letter.34 \u0026lt;dbl\u0026gt;, ## # letter.58 \u0026lt;dbl\u0026gt;, matrix.45 \u0026lt;dbl\u0026gt;, matrix.46 \u0026lt;dbl\u0026gt;, matrix.47 \u0026lt;dbl\u0026gt;, ## # matrix.55 \u0026lt;dbl\u0026gt;, rotate.3 \u0026lt;dbl\u0026gt;, rotate.4 \u0026lt;dbl\u0026gt;, rotate.6 \u0026lt;dbl\u0026gt;, ## # rotate.8 \u0026lt;dbl\u0026gt; Notice that the number of observations is equal to the number observations in the left hand dataset:\n# joined data left_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) %\u0026gt;% nrow() ## [1] 2800 # data on the left hand side bfi_fake_ids %\u0026gt;% nrow() ## [1] 2800 right_join() A right_join() keeps all observations from the data.frame on the right and grabs only the observations from the left data.frame that match the right:\nright_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) ## # A tibble: 1,525 x 45 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 5 6 6 6 6 6 5 5 1 1 2 3 ## 2 3 3 4 5 5 5 5 3 3 6 4 5 ## 3 2 5 6 5 5 4 4 6 2 4 3 6 ## 4 4 4 5 4 5 4 4 4 3 2 2 2 ## 5 1 5 6 6 6 6 6 5 1 4 3 2 ## 6 4 4 2 4 6 5 4 4 3 4 2 4 ## 7 2 5 4 6 2 2 6 3 4 5 5 6 ## 8 1 6 5 4 5 6 2 5 1 1 1 1 ## 9 1 6 6 5 6 6 6 6 1 1 6 4 ## 10 1 5 5 5 5 4 4 4 4 5 2 2 ## # … with 1,515 more rows, and 33 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt;, id \u0026lt;int\u0026gt;, reason.4 \u0026lt;dbl\u0026gt;, reason.16 \u0026lt;dbl\u0026gt;, reason.17 \u0026lt;dbl\u0026gt;, ## # reason.19 \u0026lt;dbl\u0026gt;, letter.7 \u0026lt;dbl\u0026gt;, letter.33 \u0026lt;dbl\u0026gt;, letter.34 \u0026lt;dbl\u0026gt;, ## # letter.58 \u0026lt;dbl\u0026gt;, matrix.45 \u0026lt;dbl\u0026gt;, matrix.46 \u0026lt;dbl\u0026gt;, matrix.47 \u0026lt;dbl\u0026gt;, ## # matrix.55 \u0026lt;dbl\u0026gt;, rotate.3 \u0026lt;dbl\u0026gt;, rotate.4 \u0026lt;dbl\u0026gt;, rotate.6 \u0026lt;dbl\u0026gt;, ## # rotate.8 \u0026lt;dbl\u0026gt; Notice that the number of observations is equal to the number observations in the right hand dataset:\n# joined data right_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) %\u0026gt;% nrow() ## [1] 1525 # data on the left hand side ability_fake_ids %\u0026gt;% nrow() ## [1] 1525   Full Join  A full_join() keeps all observations from both the left and right data.frames, regardless of matches:\nfull_join(bfi_fake_ids,ability_fake_ids, by = c(\u0026quot;id\u0026quot;=\u0026quot;id\u0026quot;)) ## # A tibble: 3,325 x 45 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2 4 3 4 4 2 3 3 4 4 3 3 ## 2 2 4 5 2 5 5 4 4 3 4 1 1 ## 3 5 4 5 4 4 4 5 4 2 5 2 4 ## 4 4 4 6 5 5 4 4 3 5 5 5 3 ## 5 2 3 3 4 5 4 4 5 3 2 2 2 ## 6 6 6 5 6 5 6 6 6 1 3 2 1 ## 7 2 5 5 3 5 5 4 4 2 3 4 3 ## 8 4 3 1 5 1 3 2 4 2 4 3 6 ## 9 4 3 6 3 3 6 6 3 4 5 5 3 ## 10 2 5 6 6 5 6 5 6 2 1 2 2 ## # … with 3,315 more rows, and 33 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt;, id \u0026lt;int\u0026gt;, reason.4 \u0026lt;dbl\u0026gt;, reason.16 \u0026lt;dbl\u0026gt;, reason.17 \u0026lt;dbl\u0026gt;, ## # reason.19 \u0026lt;dbl\u0026gt;, letter.7 \u0026lt;dbl\u0026gt;, letter.33 \u0026lt;dbl\u0026gt;, letter.34 \u0026lt;dbl\u0026gt;, ## # letter.58 \u0026lt;dbl\u0026gt;, matrix.45 \u0026lt;dbl\u0026gt;, matrix.46 \u0026lt;dbl\u0026gt;, matrix.47 \u0026lt;dbl\u0026gt;, ## # matrix.55 \u0026lt;dbl\u0026gt;, rotate.3 \u0026lt;dbl\u0026gt;, rotate.4 \u0026lt;dbl\u0026gt;, rotate.6 \u0026lt;dbl\u0026gt;, ## # rotate.8 \u0026lt;dbl\u0026gt; Notice that the number of observations is 3325. This number represents the total number of unique people that are either in the left hand or right hand dataset or both datasets.\n  \n Filtering Joins Remember, filtering joins only affect the observations in your data, they don’t add any new variables. You might want to do a filtering join if you want to work with the obseravtions that appear in another dataset but you are not actually interested in using any of the variables in the other dataset. You might also do a filtering join to figure out why a join didn’t work.\n Semi Join  A semi_join() simply keeps all observations that appear in left dataset that have a match in the right dataset. This is exactly the same as inner_join() except we didn’t add any variables to the dataset:\nsemi_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) ## # A tibble: 1,000 x 29 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 5 4 5 4 4 4 5 4 2 5 2 4 ## 2 2 3 3 4 5 4 4 5 3 2 2 2 ## 3 2 5 6 6 5 6 5 6 2 1 2 2 ## 4 4 4 5 4 3 5 4 5 4 6 1 2 ## 5 1 6 6 1 5 5 4 4 2 3 1 2 ## 6 4 5 5 6 5 5 5 4 1 1 3 2 ## 7 1 6 6 1 6 5 2 5 1 1 1 1 ## 8 2 4 4 4 3 6 5 6 1 1 2 4 ## 9 2 4 5 6 5 4 6 4 2 4 2 2 ## 10 5 3 5 4 2 2 2 4 3 4 3 4 ## # … with 990 more rows, and 17 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt;, id \u0026lt;int\u0026gt; Notice how the number of observations between the two joins are equal:\nsemi_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) %\u0026gt;% nrow() ## [1] 1000 inner_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) %\u0026gt;% nrow() ## [1] 1000   Inner Join  An anti_join() drops all the rows in the left dataset that have a match in the right dataset. In our case, the anti_join() will give us a dataset with all the participants that completed the BFI but did not complete the cognitive assessment for our ability dataset:\nanti_join(bfi_fake_ids, ability_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) ## # A tibble: 1,800 x 29 ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2 4 3 4 4 2 3 3 4 4 3 3 ## 2 2 4 5 2 5 5 4 4 3 4 1 1 ## 3 4 4 6 5 5 4 4 3 5 5 5 3 ## 4 6 6 5 6 5 6 6 6 1 3 2 1 ## 5 2 5 5 3 5 5 4 4 2 3 4 3 ## 6 4 3 1 5 1 3 2 4 2 4 3 6 ## 7 4 3 6 3 3 6 6 3 4 5 5 3 ## 8 4 4 5 6 5 4 3 5 3 2 1 3 ## 9 2 5 5 5 5 5 4 5 4 5 3 3 ## 10 5 5 5 6 4 5 4 3 2 2 3 3 ## # … with 1,790 more rows, and 17 more variables: E3 \u0026lt;int\u0026gt;, E4 \u0026lt;int\u0026gt;, ## # E5 \u0026lt;int\u0026gt;, N1 \u0026lt;int\u0026gt;, N2 \u0026lt;int\u0026gt;, N3 \u0026lt;int\u0026gt;, N4 \u0026lt;int\u0026gt;, N5 \u0026lt;int\u0026gt;, O1 \u0026lt;int\u0026gt;, ## # O2 \u0026lt;int\u0026gt;, O3 \u0026lt;int\u0026gt;, O4 \u0026lt;int\u0026gt;, O5 \u0026lt;int\u0026gt;, gender \u0026lt;int\u0026gt;, education \u0026lt;int\u0026gt;, ## # age \u0026lt;int\u0026gt;, id \u0026lt;int\u0026gt; Notice that the number of observations is 1800. This number represents the total number of people that completed the BFI assessment but did not complete the cognitive assessment.\nanti_join(ability_fake_ids,bfi_fake_ids, by = c(\u0026quot;id\u0026quot; = \u0026quot;id\u0026quot;)) ## # A tibble: 525 x 17 ## reason.4 reason.16 reason.17 reason.19 letter.7 letter.33 letter.34 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1 0 1 1 1 ## 2 1 1 1 1 1 1 1 ## 3 1 1 1 0 0 0 0 ## 4 1 1 1 1 1 1 1 ## 5 0 0 0 1 0 1 1 ## 6 1 NA 1 NA 1 1 1 ## 7 1 1 1 1 1 0 1 ## 8 1 1 1 1 0 0 1 ## 9 0 1 1 1 1 1 1 ## 10 1 1 1 1 1 0 1 ## # … with 515 more rows, and 10 more variables: letter.58 \u0026lt;dbl\u0026gt;, ## # matrix.45 \u0026lt;dbl\u0026gt;, matrix.46 \u0026lt;dbl\u0026gt;, matrix.47 \u0026lt;dbl\u0026gt;, matrix.55 \u0026lt;dbl\u0026gt;, ## # rotate.3 \u0026lt;dbl\u0026gt;, rotate.4 \u0026lt;dbl\u0026gt;, rotate.6 \u0026lt;dbl\u0026gt;, rotate.8 \u0026lt;dbl\u0026gt;, ## # id \u0026lt;int\u0026gt; Notice that the number of observations is 525. This number represents the total number of people that completed the cognitive assessment but did not complete the BFI assessment.\n  \n .collapse-toggle{ text-decoration: none; padding: 8px 16px; font-size: 18px; } #verbs  h4 { width: 100%; background: #f4f4f4; margin: 2px 0; }    "
 },
 {
   "uri": "/posts/restructuring-dyadic-data/",
   "title": "Restructuring Dyadic Data",
   "tags": ["code", "dplyr", "R"],
   "description": "",
   "content": "Introduction \u0026amp; Setup This post will explain how to restructure data without a Graphical User Interface (GUI) program e.g. David Kenny’s website. Instead, I will show you how to manipulate data both in a general sense and specifically for dyadic data analysis in a more flexible way by writing my own code and sharing it with you. Be forewarned, writing R-scripts can be frustrating but the payoffs are huge. I highly encourage you to move away from GUIs and start writing scripts for many, many reasons that I won’t discuss here.\nTo manipulate and restructure data, I will be using some really useful R-packages that all have the same underlying philosophy (mostly because they were written by the same guy: Hadley Wickham). I would encourage you to reference his book R for Data Science, which explains some very useful theory on tidy data, research workflows, and tools that help execute a data analysis project in a clean, reproducible way.\nTo setup my session I will first load the packages I will need.\nlibrary(tidyverse) library(haven) library(pixiedust)  The tidyverse Notice that I am loading a package called tidyverse. This package will load the most commonly used R-packages for importing, tidying, and transforming data. These are:\n ggplot2 for plotting tibble for working with data.frames in a more efficient way tidyr for “tidying” data (more later) readr for reading tabular data into R purrr for performing iteration over data structures dplyr for manipulating and joining data  For our purposes, we will mostly be using tidyr, dplyr, and purrr. We will also be using a package that comes with the tidyverse package but is not loaded explicitly by loading the tidyverse package: haven, which is very useful for loading SPSS, SAS, and Stata files into R. Note that when you install the tidyverse package, you will also install many other very useful packages (see below):\ntidyverse_packages() # list all packages including in the tidyverse ## [1] \u0026quot;broom\u0026quot; \u0026quot;cli\u0026quot; \u0026quot;crayon\u0026quot; \u0026quot;dplyr\u0026quot; \u0026quot;dbplyr\u0026quot; ## [6] \u0026quot;forcats\u0026quot; \u0026quot;ggplot2\u0026quot; \u0026quot;haven\u0026quot; \u0026quot;hms\u0026quot; \u0026quot;httr\u0026quot; ## [11] \u0026quot;jsonlite\u0026quot; \u0026quot;lubridate\u0026quot; \u0026quot;magrittr\u0026quot; \u0026quot;modelr\u0026quot; \u0026quot;purrr\u0026quot; ## [16] \u0026quot;readr\u0026quot; \u0026quot;readxl\\n(\u0026gt;=\u0026quot; \u0026quot;reprex\u0026quot; \u0026quot;rlang\u0026quot; \u0026quot;rstudioapi\u0026quot; ## [21] \u0026quot;rvest\u0026quot; \u0026quot;stringr\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;tidyr\u0026quot; \u0026quot;xml2\u0026quot; ## [26] \u0026quot;tidyverse\u0026quot;  Practice data To begin our data restructuring walk-through, I first downloaded practice data from David Kenny’s data restructuring link. Reading through this webpage, it is clear that there are three different ways you might need to restructure data for dyadic data analysis:\nConverting individual data to dyadic Converting from individual data to pairwise Converting dyadic data to pairwise data  David Kenny’s website provides some input data for his data restructuring GUI programs. This is nice because I can download the input data and the output data and check the “correct” output data file against the one that I will produce later.\nFirst, I downloaded all the data and put them in the same folder where I am conducting my analysis. The starting individual level data file is a SPSS file whereas the pairwise and dyadic data files are comma separated value files (.csv). Below I download the files and get them into R. Note how nice it is to work with many and potentially diverse files in one place.\n# Individual level data: indv \u0026lt;- read_sav(\u0026quot;http://davidakenny.net/kkc/c1/indiv.sav\u0026quot;) %\u0026gt;% # Use read_sav for SPSS files rename(dyad_id = dyad) %\u0026gt;% rename_all(tolower) # Dyadic level data dyad \u0026lt;- read_csv(\u0026quot;http://davidakenny.net/progs/dyad.csv\u0026quot;,col_types = \u0026quot;dddddddddd\u0026quot;) %\u0026gt;% rename_all(tolower) # Pairwise level data: pair \u0026lt;- read_csv(\u0026quot;http://davidakenny.net/progs/pairwise.csv\u0026quot;, col_types = \u0026quot;ddddddddddddd\u0026quot;) %\u0026gt;% rename_all(tolower)  1. Individual to Dyadic Going from individual level data to dyadic level data is probably the most straightforward task of the three outlined above. With individual level data, each row represents a “case” or individual. Importantly, each individual is nested within a dyad, as indicated by the dyad_id column below.\n dyad_id  gender  self1  self2  self3  self4  betw    3  1  4  4  3  4  5    3  -1  5  5  5  5  5    10  1  3  4  4  5  6    10  -1  4  5  4  5  6    11  1  4  3  5  5  8    11  -1  5  5  5  5  8    17  1  3  3  4  5  11    17  -1  4  4  4  4  11    21  1  4  4  5  5  22    21  -1  3  5  5  4  22    Note that our task will be to take the cells highlighted in dark gray and make new columns that will become our “partner” data. When we do this, we will be cutting out these rows entirely and our total N will be cut in half when we do this. Note that this is possible because our new columns will be named in a way that differentiates partners from actors thus eliminating our need for the gender column. Since dyad_id is redundant, we only need one row per dyad to distinguish our cases. Thus our resulting data should look like the table below.\nNote that the dark gray cells are the same rows from the individual level data rows in dark gray above.\n dyad_id  self1_w  self2_w  self3_w  self4_w  self1_h  self2_h  self3_h  self4_h  betw    3  5  5  5  5  4  4  3  4  5    10  4  5  4  5  3  4  4  5  6    11  5  5  5  5  4  3  5  5  8    17  4  4  4  4  3  3  4  5  11    21  3  5  5  4  4  4  5  5  22    So let’s actually perform this operation using R functions from the tidyverse package. Below is the code that I used to convert individual data to dyad level data:\nindv_dyad \u0026lt;- indv %\u0026gt;% # To be explained soon ;) arrange(dyad_id) %\u0026gt;% # gather(key,value,-dyad_id,-betw,-gender) %\u0026gt;% # mutate(gender = ifelse(gender == 1,\u0026quot;h\u0026quot;,\u0026quot;w\u0026quot;)) %\u0026gt;% # unite(new_key,key,gender,sep = \u0026quot;_\u0026quot;,remove=T) %\u0026gt;% # spread(new_key,value) # Let’s walk through the code in steps.\nFirst, I’m taking the starting data indv and telling dplyr to use the function arrange(). I pass the variable dyad_id to arrange() to tell dplyr to sort the columns from the lowest to highest dyad_id. This step is unnecessary but I like to arrange data in ways that make sense so I can better reason about the data and the functions I will need to call in order to complete a given data manipulation task.\nNext, I use the function gather() from the tidyr package. This function is powerful; it takes your data set and rearranges it into “key-value pairs”. The most basic action gather() performs is taking your entire data set and creating two columns: one for the key name and the other for the values. The first two arguments for gather() are key and value. These are simply arbitrary names that will label the two columns I described above. Next, you can indicate columns that you don’t want to gather by typing the column name with an - in front of it.\nFor our case, I simply named our key and value columns “key” and “value” (remember these are arbitrary). Then I told gather, don’t gather our dyad_id, betw, and gender columns. This simply means that they will be repeated however many times necessary. For our data, we are gathering all the “self” variables (there are 4 * 2 people within each dyad = 8 rows per dyad), thus all of our columns with a - sign will be repeated 8 times per dyad.\nindv %\u0026gt;% # \u0026lt;------------ Original data frame arrange(dyad_id) %\u0026gt;% # Sort by the dyad_id column gather(key,value, # Gather data into key and value columns -dyad_id,# | -betw, # | Do NOT gather these columns, -gender) # | repeat them instead  dyad_id  gender  key  value    3  1  self1  4    3  -1  self1  5    3  1  self2  4    3  -1  self2  5    3  1  self3  3    3  -1  self3  5    3  1  self4  4    3  -1  self4  5    10  1  self1  3    10  -1  self1  4    10  1  self2  4    10  -1  self2  5    10  1  self3  4    10  -1  self3  4    10  1  self4  5    10  -1  self4  5    11  1  self1  4    11  -1  self1  5    11  1  self2  3    11  -1  self2  5    11  1  self3  5    11  -1  self3  5    11  1  self4  5    11  -1  self4  5    17  1  self1  3    17  -1  self1  4    17  1  self2  3    17  -1  self2  4    17  1  self3  4    17  -1  self3  4    17  1  self4  5    17  -1  self4  4    21  1  self1  4    21  -1  self1  3    21  1  self2  4    21  -1  self2  5    21  1  self3  5    21  -1  self3  5    21  1  self4  5    21  -1  self4  4    The next step is to make our key column more specific. That is, as it currently stands, the rows of the the key column only differentiate between each self variable (e.g. self1, self2, etc.). We need this column to differentiate between which self question AND which partner answered the that particular self question. Luckily, we have that information in our gender variable. Our next task is to add gender information to our key column. Note that for some reason Dr. Kenny uses the suffixes \u0026quot;_h\u0026quot; and \u0026quot;_w\u0026quot; to differentiate actor and partner self scores so we’ll stick with that.\nOn the line after our call to gather(), I use a function called mutate(), which creates a new column (i.e. variable) as a function of other columns (variables). Here I just want to match Dr. Kenny’s example so I’m going to change gender to be coded as 1 = \u0026quot;h\u0026quot; and -1 = \u0026quot;w\u0026quot;. This is achieved by using mutate() and using the expression gender = ifelse(gender==1, \u0026quot;h\u0026quot;,\u0026quot;w\u0026quot;) inside mutate. This tells R to replace gender with the result of our ifelse call. ifelse() is useful because it takes a logical condition as it’s first argument, in our case gender==1, for each row ifelse() checks to see if that condition is true. If it is, it replaces the value for that row with the second argument of the ifelse(), in our case \u0026quot;h\u0026quot;. The third argument of ifelse() controls what happens if the condition is not met (i.e. FALSE), for example if gender is not equal to 1. Here we said we want to ifelse() to replace gender with \u0026quot;w\u0026quot; when gender==1 (i.e. when gender==-1).\nNow that we have our newly recoded gender column, we can unite() gender with our key column. When we call unite(), we are asking R to do exactly what it sounds like, “unite” the values of gender with key into a new column called new_key. Note that we can specify and separator string, in our case \u0026quot;_\u0026quot; which will separate the values in key from gender. See below:\nNote that the default behavior for unite() is to remove the original columns that were used to make the newly united column. This is usually a good idea. Here I’ve kept them for visualization purposes.\nindv %\u0026gt;% # \u0026lt;------------ Original data frame arrange(dyad_id) %\u0026gt;% # Sort by the dyad_id column gather(key,value, # Gather data into key and value columns -dyad_id, # \u0026lt; -betw, # | Do NOT gather these columns, repeat them instead -gender) %\u0026gt;% # \u0026lt; unite(new_key, # \u0026lt; key, # | Create a new variable, \u0026quot;new_key\u0026quot;, gender, # | by combining the values of sep = \u0026quot;_\u0026quot;, # | \u0026quot;key\u0026quot; and \u0026quot;gender\u0026quot; remove=F) # \u0026lt;----- This is normally set to TRUE  dyad_id  key  gender  new_key  value    3  self1  h  self1_h  4    3  self1  w  self1_w  5    3  self2  h  self2_h  4    3  self2  w  self2_w  5    3  self3  h  self3_h  3    3  self3  w  self3_w  5    3  self4  h  self4_h  4    3  self4  w  self4_w  5    10  self1  h  self1_h  3    10  self1  w  self1_w  4    10  self2  h  self2_h  4    10  self2  w  self2_w  5    10  self3  h  self3_h  4    10  self3  w  self3_w  4    10  self4  h  self4_h  5    10  self4  w  self4_w  5    11  self1  h  self1_h  4    11  self1  w  self1_w  5    11  self2  h  self2_h  3    11  self2  w  self2_w  5    11  self3  h  self3_h  5    11  self3  w  self3_w  5    11  self4  h  self4_h  5    11  self4  w  self4_w  5    17  self1  h  self1_h  3    17  self1  w  self1_w  4    17  self2  h  self2_h  3    17  self2  w  self2_w  4    17  self3  h  self3_h  4    17  self3  w  self3_w  4    17  self4  h  self4_h  5    17  self4  w  self4_w  4    21  self1  h  self1_h  4    21  self1  w  self1_w  3    21  self2  h  self2_h  4    21  self2  w  self2_w  5    21  self3  h  self3_h  5    21  self3  w  self3_w  5    21  self4  h  self4_h  5    21  self4  w  self4_w  4    The last step in our process is to spread() our new_key column into new columns and use the value column to fill up the cells in these new columns. Remember that our new_key column now contains information about actors and partners as well as each of the 4 self variables. Each of these names, for example self1_h will become a new column after we use spread(). See below:\nindv %\u0026gt;% # \u0026lt;------------ Original data frame arrange(dyad_id) %\u0026gt;% # Sort by the dyad_id column gather(key,value, # Gather data into key and value columns -dyad_id, # \u0026lt; -betw, # | Do NOT gather these columns, repeat them instead -gender) %\u0026gt;% # \u0026lt; unite(new_key, # \u0026lt; key, # | Create a new variable, \u0026quot;new_key\u0026quot;, gender, # | by combining the values of sep = \u0026quot;_\u0026quot;, # | \u0026quot;key\u0026quot; and \u0026quot;gender\u0026quot; remove=F) %\u0026gt;% # \u0026lt; spread(new_key, # Spread the values of \u0026quot;key\u0026quot; into new columns and value) # fill the cells of these columns with the values of \u0026quot;value\u0026quot;  dyad_id  self1_w  self2_w  self3_w  self4_w  self1_h  self2_h  self3_h  self4_h    3  5  5  5  5  4  4  3  4    10  4  5  4  5  3  4  4  5    11  5  5  5  5  4  3  5  5    17  4  4  4  4  3  3  4  5    21  3  5  5  4  4  4  5  5    Notice how our newly created data set looks identical to the original dyad data downloaded from Dr. Kenny’s website. Let’s do a formal check. Using the dplyr function setequal(), we can check to see if there exactly the same number of columns (with the same names) and exactly the same number of rows (with the same exact values):\nsetequal(dyad,indv_dyad) # Match the two data sets, are they equal? ## [1] TRUE They are! YAY!\n 2. Individual to Pairwise Now that we know how to transform data from the individual level format to a dyadic one, let’s go over how to go from an individual level format to a pairwise format. Recall that in individual level data sets, we have one row per individual that is nested within a dyad. In pairwise data structures, we will keep this same general structure. Specifically, our input data file and output data file will have the same number of rows (i.e. the same N). The critical difference is that each row will represent BOTH the actor and partner data. That is, each individual’s data will be reflected as actor variables for that specific individual’s original row but will be reflected as partner data in that specific person’s partner row.\nTo illustrate, let’s look at the original individual data set again:\n dyad_id  gender  self1  self2  self3  self4  betw    3  1  4  4  3  4  5    3  -1  5  5  5  5  5    10  1  3  4  4  5  6    10  -1  4  5  4  5  6    11  1  4  3  5  5  8    11  -1  5  5  5  5  8    17  1  3  3  4  5  11    17  -1  4  4  4  4  11    21  1  4  4  5  5  22    21  -1  3  5  5  4  22    Note that we only have one set of “self” variables but each person has unique scores on these variables in their respective rows. What we need to do is add new columns reflecting partner data but maintain the same number of rows and inserting data from each dyad member’s partner into their row. Here is the output file from Dr. Kenny’s website:\n dyad_id  partnum  betw  gender_a  self1_a  self2_a  self3_a  self4_a  gender_p  self1_p  self2_p  self3_p  self4_p    3  1  5  1  4  4  3  4  -1  5  5  5  5    3  2  5  -1  5  5  5  5  1  4  4  3  4    10  1  6  1  3  4  4  5  -1  4  5  4  5    10  2  6  -1  4  5  4  5  1  3  4  4  5    11  1  8  1  4  3  5  5  -1  5  5  5  5    11  2  8  -1  5  5  5  5  1  4  3  5  5    17  1  11  1  3  3  4  5  -1  4  4  4  4    17  2  11  -1  4  4  4  4  1  3  3  4  5    21  1  22  1  4  4  5  5  -1  3  5  5  4    21  2  22  -1  3  5  5  4  1  4  4  5  5    Notice how the data values in rows shaded in either dark gray or light gray are flipped across variables with the suffix \u0026quot;_a\u0026quot; and \u0026quot;_p\u0026quot;. This is how the data look when every person’s data is reflected as both actor data and partner data. We have the same N as before, however, we have 5 new variables that reflect each partner’s data.\nTo see how to do this in R, we need to touch on some of the same concepts as before with Individual to Dyadic transformations. However, because systematically flipping certain pairs of rows and using them to create new columns is a relatively rare thing, I had to write some special code but I think it still fits within our tidyverse framework discussed this far.\nBelow is the code I used to transform the individual data from Dr. Kenny’s website to a pairwise format. Let’s walk through it:\nindv_pair \u0026lt;- indv %\u0026gt;% split(.$dyad_id) %\u0026gt;% map_df(function(x){ person1 \u0026lt;- x %\u0026gt;% mutate(act.par = ifelse(gender == 1,\u0026quot;a\u0026quot;,\u0026quot;p\u0026quot;)) %\u0026gt;% gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% unite(new_key,key,act.par) %\u0026gt;% spread(new_key,value) person2 \u0026lt;- x %\u0026gt;% mutate(act.par = ifelse(gender == 1,\u0026quot;p\u0026quot;,\u0026quot;a\u0026quot;)) %\u0026gt;% gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% unite(new_key,key,act.par) %\u0026gt;% spread(new_key,value) bind_rows(person1,person2) }) %\u0026gt;% mutate(partnum = ifelse(gender_a == 1,1,2)) %\u0026gt;% select(dyad_id,partnum,betw,ends_with(\u0026quot;_a\u0026quot;),ends_with(\u0026quot;_p\u0026quot;)) First, I want to take the original indv data set we worked with in the last walk-through. Here however, I’m going to use a function called split(), which will take my data and create mini-data sets based on a grouping variable. Here I want split to split my data according to dyad_id. Note that, because split() is not a tidyverse function and because I am using the pipe operator i.e %\u0026gt;%, I needed to supply split() with . and the index operator $ to find the variable dyad_id within the indv data set. This tells split() which variable within indv I should split the data by, in this case dyad_id. Here is the result:\nindv %\u0026gt;% split(.$dyad_id)  dyad_id  gender  self1  self2  self3  self4  betw    3  1  4  4  3  4  5    3  -1  5  5  5  5  5    dyad_id  gender  self1  self2  self3  self4  betw    10  1  3  4  4  5  6    10  -1  4  5  4  5  6    dyad_id  gender  self1  self2  self3  self4  betw    11  1  4  3  5  5  8    11  -1  5  5  5  5  8    dyad_id  gender  self1  self2  self3  self4  betw    17  1  3  3  4  5  11    17  -1  4  4  4  4  11    dyad_id  gender  self1  self2  self3  self4  betw    21  1  4  4  5  5  22    21  -1  3  5  5  4  22   \nNext, we will use the map() function from the purrr package, which is part of the tidyverse. map() is another very powerful and flexible function that applies a function to each element of a list or data.frame. Here, map() will be applying a function to each of those mini-datasets split() created. That is, the result of split() is a list, which can hold anything inside them, of data.frames. Since there is no explicit function for performing pairwise data restructuring, we are going to make our own function. This is where the function map() becomes very flexible. It can apply a ready made function to every element of a list or data frame or you can define your own within the call to map(), which is what I’m going to do.\nNote that I have the suffix _df at the end of map. This simply means that I want map() to make sure that the result is a data.frame and nothing else. If my function does not return a data.frame, map() will throw an error telling me so. Normally, the first argument to map() is a list or data.frame but remember we are piping in the list of data.frame's that split() produced by cutting up indv by dyad_id.\nNext, we tell map() what function to perform to each of our mini-datasets. I use function(x) to say that I want to define a new function and it will take the argument x. The first thing I want to do is take x and do some stuff to it and call it person1. Note that map() is going to iterate over our list of data.frames and this means that inside our function x represents each individual mini-dataset we created.\nBecause we are flipping data around, I’m first going to have map() take each mini-dataset and create a new variable using mutate() called act.par. I’m going to use ifelse() to created act.par based on gender. If gender==1, I want act.par==\u0026quot;a\u0026quot; and if not I want act.par==\u0026quot;p\u0026quot;. I’m using “a” and “p” to refer to actor and partner, respectively. Then I’m going to do some familiar things with gather(), unite(), and spread(). Essentially, I’m gathering up all variables except dyad_id, betw, and act.par (which will get repeated). Then combining key and act.par and spreading those columns back out. This will result in our mini-dataset having 1 row.\nThen I repeat this process for a new object called person2. This time, however, ifelse() is flipping it’s conditions such that if gender==1 it get’s replaced with “p” and “a” if gender==-1. For each mini-dataset, I have two objects, person1 and person2. All, I need to do now is put person1 and person2 together and I have a pairwise mini-dataset.\nHere is what the result would look like for one dyad mini-dataset:\nindv %\u0026gt;% split(.$dyad_id) %\u0026gt;% map_df(function(x){ person1 \u0026lt;- x %\u0026gt;% mutate(act.par = ifelse(gender == 1,\u0026quot;a\u0026quot;,\u0026quot;p\u0026quot;)) %\u0026gt;% gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% unite(new_key,key,act.par) %\u0026gt;% spread(new_key,value) person2 \u0026lt;- x %\u0026gt;% mutate(act.par = ifelse(gender == 1,\u0026quot;p\u0026quot;,\u0026quot;a\u0026quot;)) %\u0026gt;% gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% unite(new_key,key,act.par) %\u0026gt;% spread(new_key,value) bind_rows(person1,person2) }) Person 1 (gathered)\n dyad_id  betw  new_key  value    3  5  gender_a  1    3  5  gender_p  -1    3  5  self1_a  4    3  5  self1_p  5    3  5  self2_a  4    3  5  self2_p  5    3  5  self3_a  3    3  5  self3_p  5    3  5  self4_a  4    3  5  self4_p  5    Person 1 (spread out)  dyad_id  betw  gender_a  self1_a  self2_a  self3_a  self4_a  gender_p  self1_p  self2_p  self3_p  self4_p    3  5  1  4  4  3  4  -1  5  5  5  5   \nPerson 2 (gathered)\n dyad_id  betw  new_key  value    3  5  gender_p  1    3  5  gender_a  -1    3  5  self1_p  4    3  5  self1_a  5    3  5  self2_p  4    3  5  self2_a  5    3  5  self3_p  3    3  5  self3_a  5    3  5  self4_p  4    3  5  self4_a  5    Person 2 (spread out)\n dyad_id  betw  gender_a  self1_a  self2_a  self3_a  self4_a  gender_p  self1_p  self2_p  self3_p  self4_p    3  5  -1  5  5  5  5  1  4  4  3  4    Combined mini-dataset\n dyad_id  betw  gender_a  self1_a  self2_a  self3_a  self4_a  gender_p  self1_p  self2_p  self3_p  self4_p    3  5  1  4  4  3  4  -1  5  5  5  5    3  5  -1  5  5  5  5  1  4  4  3  4    Recall that a convenient quality of the purrr package’s map() functions is that you can supply a suffix to map() such as map_df() and that particular map() function will be sure to give you a data.frame as a result. This means that, although split() gave us a list, this list was comprised of data.frames so map_df() will automatically combine all of our mini-datasets back into one larger dataset. The result will be our final pairwise-transformed data set. The last two lines just adds a new partnum variable to help us remember who is who and then I simply order the variables according the order that Dr. Kenny has them ordered:\n dyad_id  partnum  betw  gender_a  self1_a  self2_a  self3_a  self4_a  gender_p  self1_p  self2_p  self3_p  self4_p    3  1  5  1  4  4  3  4  -1  5  5  5  5    3  2  5  -1  5  5  5  5  1  4  4  3  4    10  1  6  1  3  4  4  5  -1  4  5  4  5    10  2  6  -1  4  5  4  5  1  3  4  4  5    11  1  8  1  4  3  5  5  -1  5  5  5  5    11  2  8  -1  5  5  5  5  1  4  3  5  5    17  1  11  1  3  3  4  5  -1  4  4  4  4    17  2  11  -1  4  4  4  4  1  3  3  4  5    21  1  22  1  4  4  5  5  -1  3  5  5  4    21  2  22  -1  3  5  5  4  1  4  4  5  5    Is our new pairwise dataset identical to Dr. Kenny’s?\nsetequal(pair,indv_pair) ## [1] TRUE It is. It is indeed. ;)\n 3. Dyad to Pairwise The final case where you might need to restructure your data from a dyadic structure to a pairwise structure. To do this transformation, we will simply do some reverse engineering of the transformations we’ve already performed. Note that at this point in the tutorial, you’ve already learned a lot about how to do different transformations using tidyverse packages and functions. Now we just need to apply the same skills we’ve used already to a new situation.\nRecall that our dyad data structure has half as many rows as our individual level data. Each row represents a dyad and we have two sets of self ratings - one for the actor and the other for the partner - denoted with a suffix \u0026quot;_a\u0026quot; or \u0026quot;_p\u0026quot;. See below:\n dyad_id  self1_w  self2_w  self3_w  self4_w  self1_h  self2_h  self3_h  self4_h  betw    3  5  5  5  5  4  4  3  4  5    10  4  5  4  5  3  4  4  5  6    11  5  5  5  5  4  3  5  5  8    17  4  4  4  4  3  3  4  5  11    21  3  5  5  4  4  4  5  5  22    To move from this dyadic data structure to a pairwise data structure, we need to expand this data again so that we have one row per person (i.e. double the N) but we need to keep both sets of “self” variables for actors and partners. Below is the code I use to move from a dyadic data structure to a pairwise structure:\ndyad_pair \u0026lt;- dyad %\u0026gt;% gather(key,value,-dyad_id,-betw) %\u0026gt;% mutate(gender = ifelse(str_detect(key,\u0026quot;_h\u0026quot;),1,-1), key = str_replace(key,\u0026quot;_w|_h\u0026quot;,\u0026quot;\u0026quot;)) %\u0026gt;% spread(key,value) %\u0026gt;% split(.$dyad_id) %\u0026gt;% map_df(function(x){ person1 \u0026lt;- x %\u0026gt;% mutate(act.par = ifelse(gender == 1,\u0026quot;a\u0026quot;,\u0026quot;p\u0026quot;)) %\u0026gt;% gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% unite(key,key,act.par) %\u0026gt;% spread(key,value) person2 \u0026lt;- x %\u0026gt;% mutate(act.par = ifelse(gender == 1,\u0026quot;p\u0026quot;,\u0026quot;a\u0026quot;)) %\u0026gt;% gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% unite(key,key,act.par) %\u0026gt;% spread(key,value) bind_rows(person1,person2) }) %\u0026gt;% mutate(partnum = ifelse(gender_a == 1,1,2)) %\u0026gt;% select(dyad_id,partnum,betw,ends_with(\u0026quot;_a\u0026quot;),ends_with(\u0026quot;_p\u0026quot;)) All of this code should look very familiar. In fact, most of it is copied and pasted from our individual to pairwise data restructuring code. This is because the only real difference between dyadic to pairwise and individual to pairwise data transformation is turning dyadic data back into individual level data. After that is complete, we follow the same steps we took when we converted individual to pairwise data transformation.\nNote that going from dyadic to individual level data is an easy task because all you need to do perform the reverse actions on the dyadic data that you used to get there in the first place. Note that the functions we have been using from the tidyr package (one of the foundational packages in the tidyverse) are all reversible. For example, the function gather() and spread() actually undo each other. The same is true for unite() and a function we have not used yet, separate(); unite() puts the values of two columns together whereas separate() breaks them apart, undoing the work of unite().\nTo illustrate, let’s look at our code from the individual to dyadic data restructuring walk-through. Note steps 1-4:\nindv_dyad \u0026lt;- indv %\u0026gt;% # arrange(dyad_id) %\u0026gt;% # gather(key,value,-dyad_id,-betw,-gender) %\u0026gt;% # \u0026lt;- 1) gather mutate(gender = ifelse(gender == 1,\u0026quot;h\u0026quot;,\u0026quot;w\u0026quot;)) %\u0026gt;% # \u0026lt;- 2) Recode gender unite(new_key,key,gender,sep = \u0026quot;_\u0026quot;,remove=T) %\u0026gt;% # \u0026lt;- 3) unite gender and key spread(new_key,value) # \u0026lt;---------------------------- 4) spread your colums out And take a look at the code that we will use to go back to individual level data. We will now perform the reverse operations (the opposite functions of the above code) in reverse order (performing steps 1-4 in reverse order):\ndyad %\u0026gt;% gather(key,value,-dyad_id,-betw) %\u0026gt;% # \u0026lt;----------- Undo step 4): use \u0026#39;gather()\u0026#39; separate(key,c(\u0026quot;key\u0026quot;,\u0026quot;gender\u0026quot;),sep = \u0026quot;_\u0026quot;) %\u0026gt;% # \u0026lt;- Undo step 3): undo \u0026#39;unite()\u0026#39; mutate(gender = ifelse(gender==\u0026quot;h\u0026quot;,1,-1)) %\u0026gt;% # \u0026lt;- Undo step 2): recode gender spread(key,value) # \u0026lt;------------------------------ Undo step 1): undo gather  Here is the result of the first step, undoing spread():  dyad %\u0026gt;% gather(key,value,-dyad_id,-betw)  dyad_id  betw  key  value    3  5  self1_w  5    3  5  self2_w  5    3  5  self3_w  5    3  5  self4_w  5    3  5  self1_h  4    3  5  self2_h  4    3  5  self3_h  3    3  5  self4_h  4    10  6  self1_w  4    10  6  self2_w  5    10  6  self3_w  4    10  6  self4_w  5    10  6  self1_h  3    10  6  self2_h  4    10  6  self3_h  4    10  6  self4_h  5    11  8  self1_w  5    11  8  self2_w  5    11  8  self3_w  5    11  8  self4_w  5    11  8  self1_h  4    11  8  self2_h  3    11  8  self3_h  5    11  8  self4_h  5    17  11  self1_w  4    17  11  self2_w  4    17  11  self3_w  4    17  11  self4_w  4    17  11  self1_h  3    17  11  self2_h  3    17  11  self3_h  4    17  11  self4_h  5    21  22  self1_w  3    21  22  self2_w  5    21  22  self3_w  5    21  22  self4_w  4    21  22  self1_h  4    21  22  self2_h  4    21  22  self3_h  5    21  22  self4_h  5    Now the second step, undoing unite():  dyad %\u0026gt;% gather(key,value,-dyad_id,-betw) %\u0026gt;% separate(key,c(\u0026quot;key\u0026quot;,\u0026quot;gender\u0026quot;),sep = \u0026quot;_\u0026quot;)  dyad_id  betw  key  gender  value    3  5  self1  w  5    3  5  self2  w  5    3  5  self3  w  5    3  5  self4  w  5    3  5  self1  h  4    3  5  self2  h  4    3  5  self3  h  3    3  5  self4  h  4    10  6  self1  w  4    10  6  self2  w  5    10  6  self3  w  4    10  6  self4  w  5    10  6  self1  h  3    10  6  self2  h  4    10  6  self3  h  4    10  6  self4  h  5    11  8  self1  w  5    11  8  self2  w  5    11  8  self3  w  5    11  8  self4  w  5    11  8  self1  h  4    11  8  self2  h  3    11  8  self3  h  5    11  8  self4  h  5    17  11  self1  w  4    17  11  self2  w  4    17  11  self3  w  4    17  11  self4  w  4    17  11  self1  h  3    17  11  self2  h  3    17  11  self3  h  4    17  11  self4  h  5    21  22  self1  w  3    21  22  self2  w  5    21  22  self3  w  5    21  22  self4  w  4    21  22  self1  h  4    21  22  self2  h  4    21  22  self3  h  5    21  22  self4  h  5    Next, we recode gender back to -1 and 1:  dyad %\u0026gt;% gather(key,value,-dyad_id,-betw) %\u0026gt;% separate(key,c(\u0026quot;key\u0026quot;,\u0026quot;gender\u0026quot;),sep = \u0026quot;_\u0026quot;) %\u0026gt;% mutate(gender = ifelse(gender==\u0026quot;h\u0026quot;,1,-1))  dyad_id  betw  key  gender  value    3  5  self1  -1  5    3  5  self2  -1  5    3  5  self3  -1  5    3  5  self4  -1  5    3  5  self1  1  4    3  5  self2  1  4    3  5  self3  1  3    3  5  self4  1  4    10  6  self1  -1  4    10  6  self2  -1  5    10  6  self3  -1  4    10  6  self4  -1  5    10  6  self1  1  3    10  6  self2  1  4    10  6  self3  1  4    10  6  self4  1  5    11  8  self1  -1  5    11  8  self2  -1  5    11  8  self3  -1  5    11  8  self4  -1  5    11  8  self1  1  4    11  8  self2  1  3    11  8  self3  1  5    11  8  self4  1  5    17  11  self1  -1  4    17  11  self2  -1  4    17  11  self3  -1  4    17  11  self4  -1  4    17  11  self1  1  3    17  11  self2  1  3    17  11  self3  1  4    17  11  self4  1  5    21  22  self1  -1  3    21  22  self2  -1  5    21  22  self3  -1  5    21  22  self4  -1  4    21  22  self1  1  4    21  22  self2  1  4    21  22  self3  1  5    21  22  self4  1  5    Finally, we spread() the columns back out, undoing gather()  dyad %\u0026gt;% gather(key,value,-dyad_id,-betw) %\u0026gt;% separate(key,c(\u0026quot;key\u0026quot;,\u0026quot;gender\u0026quot;),sep = \u0026quot;_\u0026quot;) %\u0026gt;% mutate(gender = ifelse(gender==\u0026quot;h\u0026quot;,1,-1)) %\u0026gt;% spread(key,value)  dyad_id  betw  gender  self1  self2  self3  self4    3  5  -1  5  5  5  5    3  5  1  4  4  3  4    10  6  -1  4  5  4  5    10  6  1  3  4  4  5    11  8  -1  5  5  5  5    11  8  1  4  3  5  5    17  11  -1  4  4  4  4    17  11  1  3  3  4  5    21  22  -1  3  5  5  4    21  22  1  4  4  5  5    Now our dataset is back to its individual level form. To get to a pairwise data structure, we simply do exactly the same steps we performed in Individual to Pairwise tutorial. Here is the full code again:\ndyad_pair \u0026lt;- dyad %\u0026gt;% # gather(key,value,-dyad_id,-betw) %\u0026gt;% # mutate(gender = ifelse(str_detect(key,\u0026quot;_h\u0026quot;),1,-1), # Going back to individual level key = str_replace(key,\u0026quot;_w|_h\u0026quot;,\u0026quot;\u0026quot;)) %\u0026gt;% # spread(key,value) %\u0026gt;% # split(.$dyad_id) %\u0026gt;% # map_df(function(x){ # # person1 \u0026lt;- x %\u0026gt;% # mutate(act.par = ifelse(gender == 1,\u0026quot;a\u0026quot;,\u0026quot;p\u0026quot;)) %\u0026gt;% # gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% # These are the same unite(key,key,act.par) %\u0026gt;% # steps we took when spread(key,value) # we transfomred individual # to pairwise data structures person2 \u0026lt;- x %\u0026gt;% # mutate(act.par = ifelse(gender == 1,\u0026quot;p\u0026quot;,\u0026quot;a\u0026quot;)) %\u0026gt;% # gather(key,value,-dyad_id,-betw,-act.par) %\u0026gt;% # unite(key,key,act.par) %\u0026gt;% # spread(key,value) # # bind_rows(person1,person2) # }) %\u0026gt;% mutate(partnum = ifelse(gender_a == 1,1,2)) %\u0026gt;% select(dyad_id,partnum,betw,ends_with(\u0026quot;_a\u0026quot;),ends_with(\u0026quot;_p\u0026quot;))  dyad_id  partnum  betw  gender_a  self1_a  self2_a  self3_a  self4_a  gender_p  self1_p  self2_p  self3_p  self4_p    3  1  5  1  4  4  3  4  -1  5  5  5  5    3  2  5  -1  5  5  5  5  1  4  4  3  4    10  1  6  1  3  4  4  5  -1  4  5  4  5    10  2  6  -1  4  5  4  5  1  3  4  4  5    11  1  8  1  4  3  5  5  -1  5  5  5  5    11  2  8  -1  5  5  5  5  1  4  3  5  5    17  1  11  1  3  3  4  5  -1  4  4  4  4    17  2  11  -1  4  4  4  4  1  3  3  4  5    21  1  22  1  4  4  5  5  -1  3  5  5  4    21  2  22  -1  3  5  5  4  1  4  4  5  5    Finally, is our dyad_pair dataset the same as Dr. Kenny’s pairwise dataset?\nsetequal(pair,dyad_pair) ## [1] TRUE Success!!\n "
 },
 {
   "uri": "/",
   "title": "",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/about/",
   "title": "About",
   "tags": [],
   "description": "A little more about Ethan and this website.",
   "content": " About Ethan I am currently a Ph.D. student at the University of Minnesota in the department of psychology. My main research interest is developmental plasticity, or the capacity for the individual to detect and adapt the phenotype to various ecological conditions across development. More specifically, my work explores if and how various cognitive functions, such as executive functions, working memory, learning, and attention, might be shaped and potentially enhanced by growing up in harsh and unpredictable environments.\nAnother focus of mine is reproducibility and open science practices. I use R and Rstudio to create reproducible analyses, figures, and reports in hopes to increase transparency and enhance collaboration. I am particularly interested in developing workflows that make every step of the research 100% reproducible, from data collection to publication. This involves using open source software for data collection, such as jsPsych, and leveraging R and R markdown to clean, analyze, and report on results. On this site, I share some of my techniques and insights from my own experience developing such workflows.\nThis site This site was built using Hugo and blogdown using the theme Kube.\n"
 },
 {
   "uri": "/archive/",
   "title": "Archive",
   "tags": [],
   "description": "A chronological list of all content on this site.",
   "content": ""
 },
 {
   "uri": "/tags/code/",
   "title": "Code",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/research/cognition/",
   "title": "Cognitive Adaptations to Stress",
   "tags": [],
   "description": "Examing how attention, learning, memory, problem-solving, and decision-making strategies are adapted to stressful environments ",
   "content": "Stressful environments have a profound impact on children. The prevailing view is that adverse experiences in childhood impairs the mind and derails development. In contrast, this research draws on the the specialization hypothesis, which proposes that children should develop specialized cognitive abilities that are adapted to stress. This view focuses on the strengths of people from adversity instead of exclusively on their weaknesses. This project seeks to test how different learning abilities are enhanced by early adversity. This research is important because it could help identify the types of cognitive abilities and strategies that are most effective for success among students from disadvantaged backgrounds.\nThe basic question that this research asks is:\n What are the attention, learning, memory, problem-solving, and decision-making strategies that are promoted by exposures to childhood adversity?\n-(Ellis et al., 2017)\n "
 },
 {
   "uri": "/research/stress-physiology/",
   "title": "Developmental Effects on Physical Health",
   "tags": [],
   "description": "Examining how stress exposure across the lifespan impacts physical health outcomes",
   "content": ""
 },
 {
   "uri": "/tags/dplyr/",
   "title": "Dplyr",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/",
   "title": "Ethan S. Young",
   "tags": [],
   "description": "Ph.D. student, University of Minnesota",
   "content": " About Ethan About this Site Navigating this site   \nI am currently a Ph.D. student at the University of Minnesota in the department of psychology. My main research interest is developmental plasticity, or the capacity for the individual to detect and adapt the phenotype to various ecological conditions across development. More specifically, my work explores if and how various cognitive functions, such as executive functions, working memory, learning, and attention, might be shaped and potentially enhanced by growing up in harsh and unpredictable environments.\nAnother focus of mine is reproducibility and open science practices. I use R and Rstudio to create reproducible analyses, figures, and reports in hopes to increase transparency and enhance collaboration. I am particularly interested in developing workflows that make every step of the research 100% reproducible, from data collection to publication. This involves using open source software for data collection, such as jsPsych, and leveraging R and R markdown to clean, analyze, and report on results. On this site, I share some of my techniques and insights from my own experience developing such workflows.\n\n\nI built this site because I wanted a semi-permanent place to store my publications, work, research tools, and thoughts. First, a frequently updated version of my CV can be found here. For my work, I created a research and projects repository (under construction) that contains descriptions of my broad research goals by topic and (whenever I can) code and data associated with my specific projects. The big idea behind this site is to create a home for all of my research and to use it to organize and document what I have done and when I did it. Hopefully, this will provide a high resolution map of my work and orgainizing it many different ways will help me determine where I should go next.\nIn my posts section, I document some of tools that I create and code that I write in R and occasionally post case-studies that use them. These are mostly for my own memory and it\u0026rsquo;s nice to have them in one location. Finally, I also use the posts section to write out my thoughts on various topics.\nNote: This site was built using Hugo and blogdown using the theme Kube. I adapted the theme to my taste and update it using R and Rstudio. The source code for the site can be found on github.\n\n\nResearch Areas An obvous way to organize my research is by research topic. Go to the reseach section to view my work sorted by broad topics.\n Projects I\u0026rsquo;m always working on a number of specific projects and I\u0026rsquo;ve associated much of the content on this site with a particular project. The projects section lists pages by project.\n Posts I\u0026rsquo;ve learned that many (most) things don\u0026rsquo;t fit very neatly into predefined boxes or categories. The posts section of this site is a \u0026ldquo;miscellaneous\u0026rdquo; box for things that may or may not wind up being helpful.\n  Tags  I try to add tags that help group related pages together. Tagging content adds an element of flexible organization and, over time, can organically create useful structure and organization.  Archive In addition to projects and tags, I made a point to chronologically organize my work to see how it evolves over time. The section lists out my work across time for a historical perspectve the development of my research and thinking.\n Search! There\u0026rsquo;s a search bar at the top of the page. Use it to search for keywords across all the pages on this site.\n  \n\n #livetabs a { color: #000; text-decoration: none; background: white; border-radius: 0px; padding: 0; border: 4px none black; text-align:center; } #livetabs a:hover { color: #3794de; border-bottom-color: #3794de; opacity: .7; } #livetabs li.active a { background: #fff; border-color: white white #eee white; color: #3794de; border-bottom-color: #3794de; cursor: default; } #livetabs li.active a:hover { opacity: 1; } #livetabs  ul  li{ margin: auto; padding: 0px 10px; font-size: 18px; border-bottom: 2px solid black; } #livetabs  ul  li.active{ border-bottom-color: #3794de; } #livetabs  ul  li:hover{ border-bottom-color: #3794de; } .my-collapse h4 { background: white; padding: 8px 16px; margin: 1px 25% 0 25%; font-size: 15px; line-height: 24px; border-left: none; border-right: none; } .my-collapse div { border: 1px none rgba(0, 0, 0, 0.1); padding: 24px 32px 24px; margin-bottom: 0; } .my-collapse h4 a{ color: white; }  "
 },
 {
   "uri": "/cv/",
   "title": "Ethan S. Young, MA",
   "tags": [],
   "description": "Curriculum Vitae",
   "content": "Employment  Academic Positions   2013-present   PhD Student\nUniversity of Minnesota\n     Education  Graduate   2019 (expected)    Doctor of Philosophy\nUniversity of Minnesota\nMajor: Psychology\n    2017   Master's of Arts\nUniversity of Minnesota\nMajor: Psychology\n    Undergraduate   2012   Bachelor of Science\nUniversity of Minnesota\nMajors: Psychology \u0026amp; Spanish Studies\n   \n Publications  Peer Reviewed Articles  in pressYoung, E. S., Farrel, A. K., Carlson, E. A., Englund, M. M. , Miller, G. E., Gunnar, M. R., Roisman, G. I., \u0026 Simpson, J. A. (in press). The Dual Impact of Early and Concurrent Life Stress on Adult Diurnal Cortisol Patterns: A Prospective Study. Psychological Science.Abstract Major life stress often produces a flat diurnal cortisol slope, an indicator of potential long-term health problems. Exposure to stress early in childhood or the accumulation of stress across the lifespan may be responsible for this pattern. However, the relative impact of life stress during distinct life stages on diurnal cortisol is unknown. Using a longitudinal sample of adults followed from birth, we examine three stress exposure models on diurnal cortisol: the cumulative stress model (CM), the biological embedding model (BEM), and the sensitization model (SM). The CM targets cumulative life stress, the BEM implicates early childhood stress, and the SM suggests current stress interacts with early life stress to produce flat diurnal cortisol slopes. Analyses indicate that high early stress exposure and high current stress predict flat diurnal cortisol slopes, consistent with the SM. These novel findings advance our understanding of diurnal cortisol patterns and highlight avenues for intervention.\nFarrell, A. K., Waters, T. E. A., Young, E. S., Englund, M. M., Carlson, E. E., Roisman, G. I., \u0026 Simpson, J. A. (in press). Early maternal sensitivity, attachment security in young adulthood, and cardiometabolic risk at midlife. Attachment \u0026 Human Development, 0(0), 1–17.Abstract Children who experience high-quality early parenting tend to have better physical health, but limited research has tested whether this association extends into adulthood using prospective, observational assessments. Likewise, mechanisms that may explain such links have not yet been illuminated. In this study, we test whether the quality of early maternal sensitivity experienced during the first 3½ years of life predicts cardiometabolic risk at midlife (ages 37 and 39 years) via attachment representations measured in young adulthood (ages 19 and 26 years). We do so by comparing the predictive significance of two different forms of attachment representations coded from the Adult Attachment Interview (AAI): (a) secure base script knowledge and (b) coherence of mind. Using data from the Minnesota Longitudinal Study of Risk and Adaptation, we find that early maternal sensitivity is negatively associated with cardiometabolic risk at midlife. Secure base script knowledge (but not coherence of mind) partially mediated this link. These findings are consistent with the possibility that early parenting has lasting significance for physical health in part by promoting higher levels of secure base script knowledge.\n2018Young, E. S., Griskevicius, V., Simpson, J. A., Waters, T. E. A., \u0026 Mittal, C. (2018). Can an Unpredictable Childhood Environment Enhance Working Memory? Testing the Sensitized-Specialization Hypothesis. Journal of Personality and Social Psychology, 114(6), 891–908.Abstract DownloadAlthough growing up in an adverse childhood environment tends to impair cognitive functions, evolutionary-developmental theory suggests that this might be only one part of the story. A person's mind may instead become developmentally specialized and potentially enhanced for solving problems in the types of environments in which the person grew up. In the current research, we tested whether these specialized advantages in cognitive function might be sensitized to emerge in currently uncertain contexts. We refer to this as the sensitized-specialization hypothesis. We conducted experimental tests of this hypothesis in the domain of working memory, examining how growing up in unpredictable versus predictable environments affects different facets of working memory. Although growing up in an unpredictable environment is typically associated with impairments in working memory, we show that this type of environment is positively associated with those aspects of working memory that are useful in rapidly changing environments. Importantly, these effects emerged only when the current context was uncertain. These theoretically derived findings suggest that childhood environments shape, rather than uniformly impair, cognitive functions.\nMartin, J., Anderson, J. E., Groh, A. M., Waters, T. E. A., Young, E. S., Johnson, W. F., … Roisman, G. I. (2018). Maternal Sensitivity During the First 31/2 Years of Life Predicts Electrophysiological Responding to and Cognitive Appraisals of Infant Crying at Midlife. Developmental Psychology, 54(10), 1917–1927.Abstract DownloadChildren who experience high-quality early parenting tend to have better physical health, but limited research has tested whether this association extends into adulthood using prospective, observational assessments. Likewise, mechanisms that may explain such links have not yet been illuminated. In this study, we test whether the quality of early maternal sensitivity experienced during the first 3½ years of life predicts cardiometabolic risk at midlife (ages 37 and 39 years) via attachment representations measured in young adulthood (ages 19 and 26 years). We do so by comparing the predictive significance of two different forms of attachment representations coded from the Adult Attachment Interview (AAI): (a) secure base script knowledge and (b) coherence of mind. Using data from the Minnesota Longitudinal Study of Risk and Adaptation, we find that early maternal sensitivity is negatively associated with cardiometabolic risk at midlife. Secure base script knowledge (but not coherence of mind) partially mediated this link. These findings are consistent with the possibility that early parenting has lasting significance for physical health in part by promoting higher levels of secure base script knowledge.\n2017Young, E. S., Simpson, J. A., Griskevicius, V., Huelsnitz, C. O., \u0026 Fleck, C. (2017). Childhood attachment and adult personality: A life history perspective. Self and Identity, 0(0), 1–17.Abstract DownloadAccording to attachment theory, being securely attached to one’s primary caregiver early in life should be related to personality adulthood. However, no studies to date have investigated this key premise using prospective data. To address this gap, we discuss evolutionary-based models of attachment and use them to examine how secure versus insecure children might score differently on Big 5 traits that underlie the meta-trait stability. We modeled data from Minnesota Longitudinal Study of Risk and Adaptation (N = 170), which has followed participants across 30 years. Participant’s early attachment status was assessed in Ainsworth’s Strange at 12 and 18 months and personality was assessed on Big 5 measures at age 32. Being securely attached early in childhood predicted three of the Big 5 traits known to tap the meta-trait stability. Specifically, participants rated as secure early in life scored higher on agreeableness and conscientiousness and lower on neuroticism in adulthood, whereas those rated as insecure scored lower on agreeableness and conscientiousness and higher on neuroticism. Exploratory mediation analyses revealed that neither adult attachment representations nor internalizing/externalizing symptoms mediated the association between early security and stability. The implications of these findings for understanding the origins of personality variation are discussed.\nSzepsenwol, O., Griskevicius, V., Simpson, J. A., Young, E. S., Fleck, C., \u0026 Jones, R. E. (2017). The effect of predictable early childhood environments on sociosexuality in early adulthood. Evolutionary Behavioral Sciences, 11(2), 131Abstract DownloadAccording to life history theory, sociosexual orientations in adulthood should be affected by an individual’s early childhood environment. Highly predictable (sta- ble) environments should increase the potential fitness benefits of long-term (slow) mating strategies as well as the potential costs of short-term (fast) mating strategies. Experiencing a more predictable childhood environment, therefore, should lead individuals to enact a slower life history strategy characterized by more restricted sociosexual behaviors. We tested this hypothesis in the Minnesota Longitudinal Study of Risk and Adaptation (MLSRA), an ongoing longitudinal study that has followed individuals from before they were born into adulthood. Indicators of sociosexuality in early adulthood were assessed by trained coders based on inter- views conducted with participants about their current relationship, their relation- ship history, and their future relationship aspirations when they were 23 years old. The findings revealed that having experienced more predictable environments during the first 4 years of life (indexed by less frequent changes in parents’ employment status, cohabitation status, and residence) prospectively predicted more restricted sociosexuality at age 23, over and above current levels of predict- ability (that also uniquely predicted restricted sociosexuality at age 23). This early life predictability effect was partially mediated by greater early maternal support and being securely attached at age 19. Viewed together, these findings suggest that greater predictability early in life may be partially conveyed to children through more supportive parenting, which results in secure attachment in adolescence, which in turn predicts more restricted sociosexuality in early adulthood.\n2015Mittal, C., Griskevicius, V., Simpson, J. A., Sung, S., \u0026 Young, E. S. (2015). Cognitive Adaptations to Stressful Environments: When Childhood Adversity Enhances Adult Executive Function. Journal of Personality and Social Psychology, 109(4), 604–621.Abstract DownloadCan growing up in a stressful childhood environment enhance certain cognitive functions? Drawing participants from higher-income and lower-income backgrounds, we tested how adults who grew up in harsh or unpredictable environments fared on 2 types of executive function tasks: inhibition and shifting. People who experienced unpredictable childhoods performed worse at inhibition (overriding dominant responses), but performed better at shifting (efficiently switching between different tasks). This finding is consistent with the notion that shifting, but not inhibition, is especially useful in unpredictable environments. Importantly, differences in executive function between people who experienced unpredictable versus predictable childhoods emerged only when they were tested in uncertain contexts. This catalyst suggests that some individual differences related to early life experience are manifested under conditions of uncertainty in adulthood. Viewed as a whole, these findings indicate that adverse childhood environments do not universally impair mental functioning, but can actually enhance specific types of cognitive performance in the face of uncertainty.\n     Book Chapters  in pressYoung, E. S., \u0026 Simpson, J. A. (in press). A life history evolutionary perspective on relationship maintenance. In B. G. Ogolsky \u0026 J. K. Monk (Eds.), Relationship maintenance: Theory, process, and context. New York: Cambridge University Press.2017Simpson, J. A., Griskevicius, V., Szepsenwol, O., \u0026 Young, E. S. (2017). An evolutionary life history perspective on personality and mating strategies. In A. T. Church (Ed.), The Praeger handbook of personality across cultures. Volume 3: Evolutionary, ecological, and cultural contexts of personality (pp. 1-29). Santa Barbara, CA: Praeger.Download     "
 },
 {
   "uri": "/tags/ggplot2/",
   "title": "Ggplot2",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/tags/lavaan/",
   "title": "Lavaan",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/research/metabolic-traits/",
   "title": "Metabolic Traits &amp; Optimal Foraging",
   "tags": [],
   "description": "Looking at how body composition, metabilism variables, and energy consumption affects foraging behavior and risk-taking",
   "content": ""
 },
 {
   "uri": "/posts/",
   "title": "Posts",
   "tags": [],
   "description": "All my random posts, most recent first.",
   "content": ""
 },
 {
   "uri": "/projects/",
   "title": "Projects",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/tags/r/",
   "title": "R",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/research/",
   "title": "Research",
   "tags": [],
   "description": "My curated data archive, grouped by research topic.",
   "content": ""
 },
 {
   "uri": "/resources/",
   "title": "Resources",
   "tags": [],
   "description": "A list of resources I have found valuable.",
   "content": "Learning R There is so much to learn in R. There are many skills you can learn using R including data transformation and manipulation techniques, plotting, statistics (obviously), reporting, reproducibility, and many other topics. In my opinion, although the statistical packages in R are many and highly useful and effective, the point of learning R should not just be to learn/use statistics alone. Instead, you should learn R in order to learn how to program and to intimately understand how to work with data. My experience is that 80-90% of data analysis is simply preparing data for analysis (cleaning, transforming, restructuring, exploring etc.). But actually performing your analysis is relatively straightfoward. Programming makes all the steps that come before your analysis much easier and much more systematic.\nTo learn more about how to program in R, I recommend taking a look at the resources below:\n  Resource  Description      DataCamp  Website with many R courses complete with video tuturials and interactive exercises.    DataCamp Community  A section of DataCamp that is free and community driven.    From Rstudio  Rstudio’s list of online learning resources for R and other things.    R for Data Science  A great book learning how to use R for data analysis and programming    Advanced R  Another book by Hadley Wickham with many advanced programming topics for R.      R markdown R markdown is a highly useful tool for writing reproducible reports, manuscripts,, presentations, books, and websites/web applications. Its power lies in how it is able to knit together code and text to create documents. You create an R markdown file that contains code that does important stuff and can be reproducibly executed and R markdonw converts it into different formats. If you need to change your analysis, you can do so quickly in our R markdown file and any statistic, plot, or table that tied to your data will automatically update each time your compile your R markdown file.\nHere are some useful resources for R markdown:\n  Resource  Description      R Markdown  The official R markdown website.    R Markdown: The Definitive Guide  Official book for all things R markdown.    knitr  The official website for knitr, which basically powers R markdown.    Flexdashboard  Documentation on how to make dashboards in R markdown.    bookdown  Official bookdown site.    bookdown: authoring books with bookdown  Documentation for how to write books with the bookdown package.    blogdown  Documentation for how to create websites in R markdown.    pkgdown  Documentation for how to create websites for an R pacakge.    htmlWidgets  Learn how to make htmlWidgets for R markdown reports.    R Pubs  Free website that allows you to publish R markdown reports to the internet.      Tidyverse   Resource  Description      tidyverse  The official tidyverse website.    ggplot2  Awesome visualizations.    dplyr  Tools for data manipulation    tidyr  Tools to make your data tidy e.g. columns are variables, rows are observations, cells are values.    readr  Reading rectangular data.    haven  Read data from SPSS, SAS, or Stata into R    purrr  Enhancing functional programming (e.g. write better for loops).    tibble  Revamped data.frame package    stringr  Helpful package for working with strings.      Shiny   Resource  Description      shiny  The official shiny website.    shinyapps  A free place to publish shiny apps to the web.    shinydashboard  Build dashboards in shiny.    shinyjs  A useful package for extending the shiny framework.      "
 },
 {
   "uri": "/tags/sem/",
   "title": "Sem",
   "tags": [],
   "description": "",
   "content": ""
 },
 {
   "uri": "/tags/",
   "title": "Tags",
   "tags": [],
   "description": "Content grouped by tags.",
   "content": ""
 }]